[
  {
    "objectID": "spacer.html",
    "href": "spacer.html",
    "title": "spacer",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "spacer"
    ]
  },
  {
    "objectID": "inference/linearreg.html",
    "href": "inference/linearreg.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "inference/distribution.html",
    "href": "inference/distribution.html",
    "title": "Distributions",
    "section": "",
    "text": "Sampling Distributions\nKey points about modality and skewness in sampling distributions:\nModality:\n\nUnimodal: A sampling distribution with only one prominent peak.\nBimodal: A sampling distribution with two distinct peaks.\nMultimodal: A sampling distribution with more than two peaks.\nUniform: In a uniform distribution, every value within a specified range has an equal probability of occurring, creating a flat horizontal line when visualized on a graph.\n\nSkewness:\n\nPositive Skewness (Right Skewed): The tail of the distribution extends further to the right side, meaning there are a few very high values compared to the majority of lower values.\nNegative Skewness (Left Skewed): The tail of the distribution extends further to the left side, indicating a few very low values compared to most higher values.\nZero Skewness (Symmetrical): The distribution is balanced with the left and right sides mirroring each other.\n\n\n\nCode\nlibrary(ggplot2)\n\n# Unimodal\nggplot(data.frame(x = rnorm(1000)), aes(x)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  ggtitle(\"Unimodal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\n# Bimodal\nggplot(data.frame(x = c(rnorm(500, -2), rnorm(500, 2))), aes(x)) +\n  geom_density(fill = \"green\", alpha = 0.5) +\n  ggtitle(\"Bimodal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\n# Multimodal\nggplot(data.frame(x = c(rnorm(300, -4), rnorm(300, 0), rnorm(300, 4))), aes(x)) +\n  geom_density(fill = \"purple\", alpha = 0.5) +\n  ggtitle(\"Multimodal Distribution\")\n\n\n\n\n\n\n\n\n\nCode\n# Uniform\nggplot(data.frame(x = runif(1000, -5, 5)), aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"orange\", color = \"black\", alpha = 0.5) +\n  ggtitle(\"Uniform Distribution\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Positive Skew\nggplot(data.frame(x = rexp(1000, rate = 1)), aes(x)) +\n  geom_density(fill = \"red\", alpha = 0.5) +\n  ggtitle(\"Positive Skew (Right Skewed)\")\n\n\n\n\n\n\n\n\n\nCode\n# Negative Skew\nggplot(data.frame(x = -rexp(1000, rate = 1)), aes(x)) +\n  geom_density(fill = \"cyan\", alpha = 0.5) +\n  ggtitle(\"Negative Skew (Left Skewed)\")\n\n\n\n\n\n\n\n\n\nCode\n# Symmetrical\nggplot(data.frame(x = rnorm(1000)), aes(x)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  ggtitle(\"Zero Skewness (Symmetrical)\")\n\n\n\n\n\n\n\n\n\nKey Differences Between General and Sampling Distributions\n\n\n\n\n\n\n\n\n\nAspect\nGeneral Distributions\nSampling Distributions\n\n\n\n\nFocus\nDistribution of raw data or random variables\nDistribution of a statistic (e.g., sample mean, proportion)\n\n\nPopulation or Sample\nDescribes population or a single sample\nDerived from repeated sampling of a population\n\n\nExamples\nNormal, Uniform, Exponential\nSampling distribution of sample mean, proportion, etc.\n\n\nDerived From\nDirectly observed data\nRepeatedly calculated from samples\n\n\nShape\nDepends on the data (e.g., normal, skewed)\nDepends on sample size and population (Central Limit Theorem often applies)\n\n\nRole in Statistics\nDescribe the data or population characteristics\nUsed for inferential statistics (e.g., confidence intervals, hypothesis testing)\n\n\n\nExample 2.12 Student Math Performance econometrics\n\n\nCode\ndata(wage1, package='wooldridge')\n\n# Estimate log-level model\nlm( log(wage) ~ educ, data=wage1 )\n\n\n\nCall:\nlm(formula = log(wage) ~ educ, data = wage1)\n\nCoefficients:\n(Intercept)         educ  \n    0.58377      0.08274  \n\n\n\n\nFrequency Distributions for categorical data\nA frequency distribution is a table used to organize data. The left column (called classes or groups) includes all possible responses on a variable being studied. The right column is a list of the frequencies, or number of observa tions, for each class. (see png 27813)(book278 1.3)\nRelative frequency Distribution\nis obtained by dividing each frequency by the number of observations and multiplying the resulting proportion by 100%. (see png 27813)(book278 1.3)\nFrequency Distributions for numerical data\nBook278 1.5\nSimilar to a frequency distribution for categorical data (Section1.3), a frequency distribution for numerical data is a table that summarizes data by listing the classes in the left column and the number of observations in each class in the right column. However, the classes, or intervals, for a frequency distribution of numerical data are not as easily identifiable. Determining the classes of a frequency distribution for numerical data requires an swers to certain questions: How many classes should be used? How wide should each class be?\nCumulative Frequency Distribution\nA cumulative frequency distribution contains the total number of observations whose values are less than the upper limit for each class. We construct a\ncumulative frequency distribution by adding the frequencies of all frequency distribution classes up to and including the present class. (see png 27815) (ECON278 1.5)\nRelative Cumulative Frequency Distribution\nIn a relative cumula tive frequency distribution, cumulative frequencies can be expressed as cu mulative proportions or percents. (ECON278 1.5) (see png 27815)\n\n\n\n\n27813\n\n\nConstruction of a Frequency Distribution \n\n\n\n27815\n\n\n\n\n\n\nShape of a Distribution\nWe can describe graphically the shape of the distribution by a histogram. That is, we can visually determine whether data are evenly spread from its middle or center. Sometimes the center of the data divides a graph of the distribution into two “mirror images,” so that the portion on one side of the middle is nearly identical to the portion on the other side. Graphs that have this shape are symmetric; those without this shape are asymmetric, or skewed.\nSymmetry\nThe shape of a distribution is said to be symmetric if the observations are bal anced, or approximately evenly distributed, about its center.\nSkewness\nA distribution is skewed, or asymmetric, if the observations are not sym metrically distributed on either side of the center. A skewed-right distribution (sometimes called positively skewed) has a tail that extends farther to the right. A skewed-left distribution (sometimes called negatively skewed) has a tail that extends farther to the left.\nFigure 278151, Figure 1.15(b), and Figure 1.15(c) illustrate a histogram for a continu ous numerical unimodal variable with a symmetric distribution, a skewed-right distribu tion and a skewed-left distribution, respectively.\nBox-and-Whisker Plot\nA box-and-whisker plot is a graph that describes the shape of a distribution in terms of the five-number summary: the minimum value, first quartile (25th percentile), the median, the third quartile (75th percentile), and the maximum value. The inner box shows the numbers that span the range from the first to the third quartile. A line is drawn through the box at the median. There are two “whiskers.” One whisker is the line from the 25th percentile to the mini mum value; the other whisker is the line from the 75th percentile to the maxi mum value. Using Minitab, we see in Figure 27822 the shapes of the distribution of sales for these four locations.(ECON278 2.2) (see png 27822)\n\n \n\n\n\n\nProbability Distribution Function\nThe probability distribution function, \\(P(x)\\), of a discrete random variable \\(X\\) represents the probability that \\(X\\) takes the value \\(x\\), as a function of \\(x\\). That is, \\[P(x) = P(X = x), \\text{for all values of } x\\]. We use the term probability distribution to represent probability distribution functions in this book, following the common practice.\nRequired Properties of Probability Distribution for Discrete Random Variables\nLet \\(X\\) be a discrete random variable with probability distribution \\(P(x)\\). Then,\n\n\\(0 \\leq P(x) \\leq 1\\) for any value \\(x\\), and\n\nthe individual probabilities sum to 1, that is,\n\\[\\sum_{x} P(x) = 1 \\tag{4.1}\\]\nwhere the notation indicates summation over all possible values of \\(x\\).\n\nCumulative Probability distribution\nThe cumulative probability distribution, \\(F(x_0)\\), of a random variable \\(X\\), represents the probability that \\(X\\) does not exceed the value \\(x_0\\), as a function of \\(x_0\\).\nThat is,\n\\[F(x_0) = P(X \\leq x_0) \\tag{4.2}\\]\nwhere the function is evaluated at all values of \\(x_0\\).\n\n Derived Relationship Between Probability Distribution and Cumulative Probability Distribution\nLet \\(X\\) be a random variable with probability distribution \\(P(x)\\) and cumulative probability distribution \\(F(x_0)\\). Then we can show that\n\\[F(x_0) = \\sum_{x \\leq x_0} P(x) \\tag{4.3}\\]\nwhere the notation implies that summation is over all possible values of \\(x\\) that are less than or equal to \\(x_0\\).\nDerived Properties of Cumulative Probability Distributions for Discrete Random Variables\nLet \\(X\\) be a discrete random variable with cumulative probability distribution \\(F(x_0)\\). Then we can show that:\n\n\\(0 \\leq F(x_0) \\leq 1\\) for every number \\(x_0\\), and\n\nif \\(x_0\\) and \\(x_1\\) are two numbers with \\(x_0 \\leq x_1\\), then \\(F(x_0) \\leq F(x_1)\\).\n\n\n\n\n\nThe Binomial Distribution\nSuppose that a random experiment can result in two possible mutually exclusive and collectively exhaustive outcomes, “success” and “failure,” and that \\(P\\) is the probability of a success in a single trial. If \\(n\\) independent trials are carried out, the distribution of the number of resulting successes, \\(x\\), is called the binomial distribution. Its probability distribution function for the binomial random variable \\(X = x\\) is as follows:\n\\(P(\\text{$x$ successes in $n$ independent trials})\\)\n\\[P(x) = \\frac{n!}{x!(n-x)!} P^x (1-P)^{n-x} \\tag{4.18}\\] \\[\\text{for } x = 0, 1, 2, \\ldots, n.\\]\n\nMean and Variance of a Binomial Probability Distribution\nLet \\(X\\) be the number of successes in \\(n\\) independent trials, each with a probability of success \\(P\\). Then \\(X\\) follows a binomial distribution with mean\n\\[\\mu = E[X] = nP \\tag{4.19}\\]\nand variance\n\\[\\sigma_X^2 = E[(X - \\mu_X)^2] = nP(1-P) \\tag{4.20}\\]\nThe derivation of the mean and variance of the binomial is shown in Section 4 of the chapter appendix.\n\n\n\n\nSampling Distributions\nConsider a random sample selected from a population that is used to make an inference about some population characteristic, such as the population mean, \\(\\mu\\), using a sample statistic, such as the sample mean, \\(\\bar{x}\\). We realize that every sample has different observed values and, hence, different sample means. The sampling distribution of the sample mean is the probability distribution of the sample means obtained from all possible samples of the same number of ob servations drawn from the population. Using the sampling distribution we can make an inference about the population mean. (see 27861)\n\n\n\n278611\n\n\n\n\n\n\n27861\n\n\nconlcusion to png 27861 and start of png 278611\nWe see that, although the number of years of experience for the six workers ranges from 2 to 8, the possible values of the sample mean have a range from only 3.0 to 7.5. In addition, more of the values lie in the central portion of the range. Table 6.3 presents similar results for a sample size of \\(n=5\\), and Figure 6.2 presents the graph for the sampling distribution. Notice that the means are concentrated over a narrower range. These sample means are all closer to the population mean, \\(\\mu =5.5\\). We will always find this to be true—the sampling distribution becomes concentrated closer to the population mean as the sample size increases. This important result provides an important foundation for statistical inference. In the following sections and chapters, we build a set of rigorous analysis tools on this foundation. (see 278611) (ECON278 6.1)\n\n\n\n\n\nStandard Normal Distribution for the Sample Means\nWhenever the sampling distribution of the sample means is a normal distribution, we can compute a standardized normal random variable, \\(Z\\), that has a mean of 0 and a variance of 1:\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{X - \\mu}{\\sigma / \\sqrt{n}} \\tag{6.1}\\] Results for the Sampling Distribution of the Sample Means\nLet \\(\\bar{X}\\) denote the sample mean of a random sample of \\(n\\) observations from a population with mean \\(\\mu_X\\) and variance \\(\\sigma^2\\).\n\nThe sampling distribution of \\(\\bar{X}\\) has mean:\n\\[E[\\bar{X}] = \\mu \\tag{6.2}\\]\nThe sampling distribution of \\(\\bar{X}\\) has standard deviation:\n\\[\\sigma_X = \\frac{\\sigma}{\\sqrt{n}} \\tag{6.3}\\]\nThis is called the standard error of \\(\\bar{X}\\).\nIf the sample size, \\(n\\), is not small compared to the population size, \\(N\\), then the standard error of \\(\\bar{X}\\) is as follows:\n\\[\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\sqrt{\\frac{N - n}{N - 1}} \\tag{6.4}\\]\nIf the parent population distribution is normal and, thus, the sampling distribution of the sample means is normal, then the random variable\n\\[Z = \\frac{X - \\mu}{\\sigma_{\\bar{x}}} \\tag{6.5}\\]\nhas a standard normal distribution with a mean of 0 and a variance of 1.\n\n\nFigure 6.3(27862) shows the sampling distribution of the sample means for sample sizes n = 25 and n = 100 from a normal distribution. Each distribution is centered on the mean, but as the sample size increases, the distribution becomes concentrated more closely around the population mean because the standard error of the sample mean de creases as the sample size increases. Thus, the probability that a sample mean is a fixed distance from the population mean decreases with increased sample size. (ECON278 2.2)\n\n\n\n27862\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Notebook",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "formulas.html",
    "href": "formulas.html",
    "title": "Statistics Formula Collection",
    "section": "",
    "text": "Frequency distributions\nConstructing frequency distributions where \\(k\\) is the smallest number of classes and \\(n\\) the number of observations:\n\nDecide on the number of classes \\(k\\) where \\(2^k &gt; n\\).\nDetermine the class interval \\(i\\) by \\(i \\geq \\frac{\\text{max value - min value}}{k}\\).\nSet individual class limits.\nTally the values into the classes.\nCount the number of items in each class.\n\nSample mean\n\\(\\bar{x} = \\frac{\\sum x}{n}\\)\nwhere:\n\n\\(\\bar{x}\\) denotes the sample mean.\n\\(n\\) denotes the number of values in the sample.\n\\(x\\) denotes any value.\n\nMedian\nThe midpoint of the values after they have been ordered from the minimum to the maximum values. The data must be at least an ordinal level of measurement.\n\nMode\nThe value of the observation that appears most frequently. It is especially useful in summarizing ordinal level data.\nWeighted mean\n\\(\\bar{x}_{w} = \\frac{w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n}{w_1 + w_2 + \\cdots + w_n} = \\frac{\\sum\\limits_{i=1}^n w_i x_i}{\\sum\\limits_{i=1}^n w_i}\\)\nwhere:\n\n\\(\\bar{x}_{w}\\) denotes the weighted mean.\n\\(w\\) denotes the corresponding weight.\n\nGeometric mean\nGeometric mean = \\(\\sqrt[n]{(x_{1})(x_{2}) \\cdots (x_{n})}\\)\nUseful in finding the average change of percentages, ratios, indices, or growth rates over time. The geometric mean will always be less than or equal to (never more than) the arithmetic mean. All data values must be positive. It is applied in Fisher’s Ideal Index.\nGeometric mean for a rate increase over time\nGM for a rate increase = \\(\\sqrt[n]{\\frac{\\text{value at end of period}}{\\text{value at start of period}}} - 1\\)\nUsed to find an average percentage increase over time.\nRange\nRange = maximum value - minimum value\nPopulation variance\n\\(\\sigma^2 = \\frac{\\sum (x - \\mu)^2}{N}\\)\nThe population variance \\(\\sigma^2\\) mitigates the dilemma of a single sample. In one sample, the deviation between an observed value \\(x\\) and the population mean \\(\\mu\\) might\n\n\n\n\ndiffer greatly, while in another sample the deviation might differ significantly again. \\(\\sigma^2\\) provides a measure of the average variance accounting for all samples for one unit of the population.\nwhere:\n\n\\(\\sigma^2\\) is the population variance.\n\\(x\\) is the value of a particular observation in the population.\n\\(\\mu\\) is the arithmetic mean of the population.\n\\(N\\) is the number of observations in the population.\n\nPopulation standard deviation\n\\(\\sigma = \\sqrt{\\frac{\\sum (x - \\mu)^2}{N}}\\)\nThe population standard deviation \\(\\sigma\\) mitigates the dilemma of a single sample (refer to population variance). In one sample, the deviation between an observed value \\(x\\) and the population mean \\(\\mu\\) might differ greatly, while in another sample, the deviation might differ significantly again. \\(\\sigma\\) provides a measure of the average deviation accounting for all samples for one unit of the population, in the same unit of measure as the sample.\nwhere:\n\n\\(\\sigma\\) is the population standard deviation.\n\\(x\\) is the value of a particular observation in the population.\n\\(\\mu\\) is the arithmetic mean of the population.\n\\(N\\) is the number of observations in the population.\n\nBy taking the square root of the variance, the deviation is now in the same unit of measurement as the original data.\nSample variance\n\\(s^2 = \\frac{\\sum (x - \\bar{x})^2}{n - 1}\\)\nwhere:\n\n\\(s^2\\) is the sample variance.\n\\(x\\) is the value of each observation in the sample.\n\\(\\bar{x}\\) is the mean of the sample.\n\\(n\\) is the number of observations in the sample.\nThe denominator \\((n - 1)\\) corrects its tendency for underestimation.\n\nSample standard deviation\n\\(s = \\sqrt{\\frac{\\sum (x - \\bar{x})^2}{n - 1}}\\)\nwhere:\n\n\\(s\\) is the sample standard deviation.\n\\(x\\) is the value of each observation in the sample.\n\\(\\bar{x}\\) is the mean of the sample.\n\\(n\\) is the number of observations in the sample.\nBy taking the square root of the variance, the deviation is now in the same unit of measurement as the original data.\n\n\nChebyshev’s Theorem\nFor any set of observations (sample or population), the proportion of the values that lie within k standard deviations of the mean is at least \\(1 - \\frac{1}{k^2}\\), where k is any value greater than 1.\nArithmetic mean of grouped data\n\\(\\bar{x} = \\frac{\\sum{fM}}{n}\\)\nwhere:\n\n\\(\\bar{x}\\) is the sample mean.\n\\(M\\) is the midpoint of each class.\n\\(f\\) is the frequency in each class.\n\\(n\\) is the total number of frequencies.\n\nStandard deviation of grouped data\n\\(s = \\sqrt{\\frac{\\sum f(M - \\bar{x})^2}{n - 1}}\\)\nwhere:\n\n\\(s\\) is the sample standard deviation.\n\\(M\\) is the midpoint of the class.\n\\(f\\) is the class frequency.\n\\(\\bar{x}\\) is the mean of the sample.\n\\(n\\) is the number of observations in the sample.\n\nLocation of percentile\n\\(L_{p} = (n + 1)\\frac{P}{100}\\)\nwhere:\n\n\\(P\\) is the percentile.\n\\(n\\) is the number of observations.\n\\(L_{p}\\) is the location of the percentile.\n\nCoefficient of Variation\n\\(\\text{CV} = \\frac{s}{\\bar{x}}(100)\\)\nNote: Multiplying by 100 converts the decimal to a percent.\nwhere:\n\n\\(s\\) is the sample standard deviation.\n\\(\\bar{x}\\) is the sample mean.\n\nPearson’s coefficient of skewness\n\\(\\text{sk} = \\frac{3(\\bar{x} - \\text{median})}{s}\\)\nwhere:\n\n\\(s\\) is the sample standard deviation.\n\\(\\bar{x}\\) is the sample mean.\n\n\n\n——————– page spacer ————————-\n\n\nSoftware coefficient of skewness\n\\(\\text{sk} = \\frac{n}{(n - 1)(n - 2)}\\left[\\sum\\left(\\frac{x - \\bar{x}}{s}\\right)^{3}\\right]\\)\nwhere:\n\n\\(n\\) is the number of observations in the sample.\n\\(s\\) is the sample standard deviation.\n\\(\\bar{x}\\) is the sample mean.\n\nClassical probability\n\\(\\text{Probability of an event} = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\\)\nEmpirical probability\n\\(\\text{Probability of an event} = \\frac{\\text{Number of times event occurred in the past}}{\\text{Total number of observations}}\\)\nSpecial Rule of Addition\n\\(\\text{P(A or B)} = P(A) + P(B)\\)\nEvents must be mutually exclusive.\nComplement Rule\n\\(P(A) = 1 - P(\\sim A)\\)\nEvents \\(A\\) and \\(\\sim A\\) are mutually exclusive and collectively exhaustive.\nGeneral Rule of Addition\n\\(\\text{P(A or B)} = P(A) + P(B) - P(A \\text{ and } B)\\)\nFor events that are not mutually exclusive.\nSpecial Rule of Multiplication\n\\(\\text{P(A and B)} = P(A)P(B)\\)\nRequires that two events are independent, meaning the occurrence of one event has no effect on the probability of the occurrence of the other event.\nGeneral Rule of Multiplication\n\\(\\text{P(A and B)} = \\text{P(A)}P(B \\vert A)\\)\nwhere: - \\(P(B \\vert A)\\) is the probability that \\(B\\) will occur given that \\(A\\) has already occurred (conditional probability). - The two events are not independent.\nBayes’ Theorem\n\\(P(A_{i} \\vert B) = \\frac{P(A_{i})P(B \\vert A_{i})}{P(A_{1})P(B \\vert A_{1}) + P(A_{2})P(B \\vert A_{2})}\\)\nEvents \\(A_{1}\\) and \\(A_{2}\\) are mutually exclusive and collectively exhaustive, and \\(A_{i}\\) refers to either event \\(A_{1}\\) or \\(A_{2}\\).\n\nMultiplication Formula\n\\(\\text{Total number of arrangements} = (m)(n)\\)\nPermutation\n\\({}_nP_{r} = \\frac{n!}{(n - r)!}\\)\nAny arrangement of r objects selected from a single group of n possible objects.\nCombination Formula\n\\({}_nC_{r} = \\frac{n!}{r!(n - r)!}\\)\nThe order of the selected objects is not important.\nMean of a Probability Distribution\n\\(\\mu = \\Sigma \\left[ xP(x) \\right]\\)\nwhere: - \\(\\mu\\) is the population mean. - \\(P(x)\\) is the probability. - \\(x\\) is a particular value.\nVariance of a Probability Distribution\n\\(\\sigma^{2} = \\Sigma\\left[ (x - \\mu)^{2}P(x) \\right]\\)\nwhere: - \\(\\mu\\) is the mean. - \\(P(x)\\) is the probability. - \\(x\\) is a particular value.\nBinomial Probability Distribution (with replacement)\n\\(P(x) = {}_nC_{x}\\pi^{x}(1 - \\pi)^{n - x}\\)\nwhere: - \\(C\\) denotes a combination. - \\(n\\) is the number of trials. - \\(x\\) is the number of successes. - \\(\\pi\\) is the probability of a success on each trial.\nMean of a Binomial Distribution (with replacement)\n\\(\\mu = n\\pi\\)\nwhere: - \\(\\mu\\) is the probability mean. - \\(n\\) is the number of trials. - \\(\\pi\\) is the probability of a success on each trial.\nVariance of a Binomial Distribution (with replacement)\n\\(\\sigma^{2} = n\\pi(1 - \\pi)\\)\nwhere: - \\(n\\) is the total number of trials.\n\n\n——- page spacer ——————–\n\n\n\n\\(\\pi\\) is the probability of a success on each trial.\n\nHypergeometric Distribution (without replacement)\n\\(P(x) = \\frac{({}_SC_{x})(C^{N-S}_{n-x})}{{}_NC_{n}}\\)\nwhere: - \\(N\\) is the size of the population. - \\(S\\) is the number of successes in the population. - \\(x\\) is the number of successes in the sample (may be 0, 1, 2, 3, …). - \\(n\\) is the size of the sample or the number of trials. - \\(C\\) stands for a combination.\nPoisson Distribution\n\\(P(x) = \\frac{\\mu^{x}e^{-\\mu}}{x!}\\)\nwhere: - \\(\\mu\\) is the mean number of occurrences (successes) in a particular interval. - \\(e\\) is the constant 2.71828 (base of the Naperian logarithmic system). - \\(x\\) is the number of occurrences (successes). - \\(P(x)\\) is the probability for a specified value of \\(x\\).\nMean of a Poisson Distribution\n\\(\\mu = n\\pi\\)\nwhere: - \\(\\pi\\) is the probability of success. - \\(n\\) is the total number of trials.\n**Mean of the Uniform Distribution\n\\(\\mu = \\frac{a + b}{2}\\)\nwhere: - \\(\\mu\\) is the mean.\nStandard Deviation of the Uniform Distribution\n\\(\\sigma = \\sqrt{\\frac{(b - a)^2}{12}}\\)\nwhere: - \\(a\\) is the minimum value of the interval. - \\(b\\) is the maximum value of the interval.\n**Uniform Distribution Probability\n\\(P(x) = (height)(base) = \\frac{1}{b - a}(b - a)\\)\nwhere:\n\n\n\\(a \\leq x \\leq b\\), and \\(P(x) = 0\\) elsewhere.\n\\(a\\) is the minimum value of the interval.\n\\(b\\) is the maximum value of the interval.\n\nNormal Probability Distribution\n\\(P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left[\\frac{(x - \\mu)^2}{2\\sigma^2}\\right]}\\)\nwhere: - \\(\\sigma\\) is the standard deviation. - \\(\\mu\\) is the mean. - \\(e\\) is a constant (base of the natural logarithm, approximately 2.718). - \\(\\pi\\) is a constant (approximately 3.1416). - \\(x\\) is the value of the random variable.\nStandard Normal Value (One Observation - \\(\\sigma\\) Known)\n\\(z = \\frac{x - \\mu}{\\sigma}\\)\nThis formula is used when calculating the \\(z\\) value for only one observation is calculated.\nwhere: - \\(z\\) denotes the signed distance between a selected value \\(x\\) and the population mean \\(\\mu\\), divided by the population standard deviation \\(\\sigma\\). - \\(x\\) is the value of any particular observation or measurement. - \\(\\mu\\) is the mean of the distribution. - \\(\\sigma\\) is the standard deviation of the distribution.\nContinuity Correction Factor\n\nIf \\(P(x) \\geq x\\), then use \\((x - 0.5)\\).\nIf \\(P(x) &gt; x\\), then use \\((x + 0.5)\\).\nIf \\(P(x) \\leq x\\), then use \\((x + 0.5)\\).\nIf \\(P(x) &lt; x\\), then use \\((x - 0.5)\\).\n\nThe value 0.5 is subtracted or added to a selected value when a discrete probability distribution is approximated by a continuous probability distribution.\nExponential Distribution\n\\(P(x) = \\lambda e^{-\\lambda x}\\)\nwhere: - \\(\\lambda\\) is the rate parameter, with \\(\\lambda = \\frac{1}{\\mu}\\) or \\(\\mu = \\frac{1}{\\lambda}\\). - \\(e\\) is a constant (base of the natural logarithm, approximately 2.718). - \\(x\\) is the value of the random variable. - Both the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)) are equal to \\(\\frac{1}{\\lambda}\\).\n\n\n————– page spacer ——————\n\n\nProbability of Exponential Distribution\n\\(P(\\text{arrival time $&lt; x$}) = 1 - e^{-\\lambda x}\\)\nwhere: - \\(\\lambda\\) is the rate parameter. - \\(e\\) is a constant (base of the natural logarithm, approximately 2.718). - \\(x\\) is the value of the random variable. - Both the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)) are equal to \\(\\frac{1}{\\lambda}\\).\nStandard Error of the Mean\n\\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\)\nwhere: - \\(\\sigma_{\\bar{x}}\\) is the standard deviation of the sample means indicated by \\(\\bar{x}\\). - \\(n\\) is the sample size. - \\(\\sigma\\) is the population standard deviation.\nStandard Normal Value (More than One Observation - \\(\\sigma\\) Known)\n\\(z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\)\nThis formula is used in cases where the research refers to a sample rather than just one observation.\nwhere: - \\(z\\) is the distance between a selected value, \\(\\bar{x}\\), and the population mean \\(\\mu\\), divided by the standard error of the mean \\(\\frac{\\sigma}{\\sqrt{n}}\\). - \\(\\bar{x}\\) is the sample mean. - \\(\\mu\\) is the population mean. - \\(\\sigma\\) is the population standard deviation. - \\(n\\) is the sample size.\nConfidence Interval for a Population Mean (\\(\\sigma\\) Known)\n\\(\\bar{x} \\pm z\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere: - \\(z\\) is the standardized distance from the mean \\(\\mu\\). - \\(\\bar{x}\\) is the sample mean. - \\(\\sigma\\) is the population standard deviation. - \\(n\\) is the sample size.\nConfidence Interval for a Population Mean (\\(\\sigma\\) Unknown)\n\\(\\bar{x} \\pm t \\frac{s}{\\sqrt{n}}\\)\nwhere: - \\(t\\) refers to the t distribution.\n\n\n\\(\\bar{x}\\) is the sample mean.\n\\(s\\) is the sample standard deviation.\n\\(n\\) is the sample size.\n\nSample Proportion\n\\(p = \\frac{x}{n}\\)\nwhere: - \\(p\\) is the sample proportion. - \\(x\\) is the number of successes. - \\(n\\) is the sample size.\nConfidence Interval for a Population Proportion\n\\(p \\pm z \\sqrt{\\frac{p(1-p)}{n}}\\)\nwhere: - \\(p\\) is the sample proportion, an estimate for the population proportion \\(\\pi\\). - \\(z\\) is the standard distance from the mean \\(\\mu\\). - \\(n\\) is the sample size.\nSample Size for Estimating the Population Mean\n\\(E = z\\frac{\\sigma}{\\sqrt{n}} \\quad \\text{solved for } n \\text{ yields} \\quad n = \\left(\\frac{z\\sigma}{E}\\right)^2\\)\nwhere: - \\(n\\) is the sample size. - \\(z\\) is the standard distance from the mean \\(\\mu\\). - \\(\\sigma\\) is the population standard deviation. - \\(E\\) is the maximum allowable error.\nSample Size for the Population Proportion\n\\(E = z\\sqrt{\\frac{\\pi (1 - \\pi)}{n}} \\quad \\text{solved for } n \\text{ yields} \\quad n = \\pi (1-\\pi)\\left(\\frac{z}{E}\\right)^2\\)\nwhere: - \\(n\\) is the size of the sample. - \\(z\\) is the standard normal value corresponding to the desired level of confidence. - \\(\\pi\\) is the population proportion. - \\(E\\) is the maximum allowable error.\nFinite-Population Correction Factor\n\\(FPC = \\sqrt{\\frac{N - n}{N - 1}}\\)\nTo be used if the sample is a significant part of a finite population.\nwhere: - \\(n\\) is the size of the sample. - \\(N\\) is the size of the population.\n\n\n————— page spacer ————–\n\n\nTesting a Mean (\\(\\sigma\\) Known)\n\\(z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\)\nThe \\(z\\) value is based on the sampling distribution of the sample mean \\(\\bar{x}\\), which follows the normal distribution with the sampling mean \\(\\mu_{\\bar{x}}\\) equal to the population mean \\(\\mu\\) and a standard error of the mean \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\).\nwhere: - \\(z\\) is the distance between a selected value, \\(\\bar{x}\\), and the mean \\(\\mu\\), divided by the standard error of the mean \\(\\frac{\\sigma}{\\sqrt{n}}\\). - \\(\\bar{x}\\) is the sample mean. - \\(\\mu\\) is the population mean. - \\(\\sigma\\) is the population standard deviation. - \\(n\\) is the sample size.\nTesting a Mean (\\(\\sigma\\) Unknown)\n\\(t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\\)\nwith \\(n-1\\) degrees of freedom.\nwhere: - \\(\\bar{x}\\) is the sample mean. - \\(\\mu\\) is the hypothesized population mean. - \\(s\\) is the sample standard deviation. - \\(n\\) is the number of observations in the sample.\nType II Error\n\\(z = \\frac{\\bar{x}_{C} - \\mu_{1}}{\\frac{\\sigma}{\\sqrt{n}}}\\)\nwhere: - \\(\\bar{x}_{C}\\) is the sample mean of region C. - \\(\\mu_{1}\\) is the hypothesized population mean of region C. - \\(\\sigma\\) is the population standard deviation. - \\(n\\) is the number of observations in the sample.\nTwo-Sample Test – Variance of the Distribution of Differences (\\(\\sigma\\) Known)\n\\(\\sigma^{2}_{\\bar{x_{1}} - \\bar{x_{2}}} = \\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}\\)\nwhere: - \\(\\sigma^{2}_{\\bar{x_{1}} - \\bar{x_{2}}}\\) is the variance of the differences in means. - \\(\\bar{x}_{1}\\) and \\(\\bar{x}_{2}\\) are the sample means of the first and second sample, respectively. - \\(n_{1}\\) and \\(n_{2}\\) are the sample sizes.\nTwo-Sample Test – Test of Means (\\(\\sigma\\) Known)\n\\(z = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}}}\\)\n\nThis formula applies under the following conditions: 1. The sampled populations are approximately normally distributed. 2. The sampled populations are independent. 3. The standard deviations of the two populations are known.\nwhere: - \\(z\\) is the standard value. - \\(\\bar{x}_{1}\\) and \\(\\bar{x}_{2}\\) are the sample means. - \\(\\sigma^{2}_{1}\\) and \\(\\sigma^{2}_{2}\\) are the population variances. - \\(n_{1}\\) and \\(n_{2}\\) are the sample sizes.\nTwo-Sample Test – Pooled Variance (\\(\\sigma\\) Unknown)\n\\(s^{2}_{p} = \\frac{(n_{1} - 1)s^{2}_{1} + (n_{2} - 1)s^{2}_{2}}{n_{1} + n_{2} - 2}\\)\nThis formula assumes the following conditions: 1. The sampled populations are approximately normally distributed. 2. The sampled populations are independent. 3. The standard deviations of the two populations are equal.\nThe formula computes a weighted mean of the two sample variances using the degrees of freedom that each sample provides. The resulting value serves as an estimate for the unknown population standard deviation. Pooling is used because it assumes the two populations have equal standard deviations, and combining the sample information provides the best estimate of the population standard deviation.\nwhere: - \\(s^{2}_{p}\\) is the pooled variance. - \\(s^{2}_{1}\\) and \\(s^{2}_{2}\\) are the variances of the first and second samples, respectively. - \\(n_{1}\\) and \\(n_{2}\\) are the sample sizes of the first and second samples, respectively. - \\(n_{1} + n_{2} - 2\\) is the degrees of freedom (\\(df\\)).\nTwo-Sample Test – Test of Means (\\(\\sigma\\) Unknown)\n\\(t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{s^{2}_{p}\\left(\\frac{1}{n_{1}} + \\frac{1}{n_{2}}\\right)}}\\)\nNotes - The pooled variance \\(s^{2}_{p}\\) must be computed beforehand. Prerequisites: 1. The sampled populations are approximately normally distributed. 2. The sampled populations are independent.\n\n\n———– page spacer ———————-\n\n\n\nThe standard deviations of the two populations are equal.\n\n\nThe Wilcoxon rank-sum testis an alternative to the Two-Sample t test and does notrequire normal distribution or equal variances.\n\nwhere: - \\(\\bar{x}_{1}\\): Mean of the first sample. - \\(\\bar{x}_{2}\\): Mean of the second sample. - \\(n_{1}\\): Number of observations in the first sample. - \\(n_{2}\\): Number of observations in the second sample. - \\(s^{2}_{p}\\): Pooled estimate of the population variance.\nTwo-Sample Test – Test Statistic for No Difference in Means, Unequal Variances (\\(\\sigma\\) Unknown)\n\\(t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{\\frac{s^{2}_{1}}{n_{1}} + \\frac{s^{2}_{2}}{n_{2}}}}\\)\nNote This formula assumes that both sample variances are unequal. Unlike the pooled variance approach, the denominator splits into two components using \\(s^{2}_{1}\\) and \\(s^{2}_{2}\\) as variances.\nwhere: - \\(\\bar{x}_{1}\\): Mean of the first sample. - \\(\\bar{x}_{2}\\): Mean of the second sample. - \\(s^{2}_{1}\\): Sample variance of the first sample. - \\(s^{2}_{2}\\): Sample variance of the second sample. - \\(n_{1}\\): Number of observations in the first sample. - \\(n_{2}\\): Number of observations in the second sample.\nTwo-Sample Test – Degrees of Freedom for Unequal Variance Test (\\(\\sigma\\) Unknown)\n\\(df = \\frac{\\left[\\frac{s^{2}_{1}}{n_{1}} + \\frac{s^{2}_{2}}{n_{2}}\\right]^{2}}{\\frac{\\left(\\frac{s^{2}_{1}}{n_{1}}\\right)^{2}}{n_{1} - 1} + \\frac{\\left(\\frac{s^{2}_{2}}{n_{2}}\\right)^{2}}{n_{2} - 1}}\\)\nwhere: - \\(df\\) is the degree of freedom. - \\(s^{2}_{1}\\) is the sample variance of the first sample. - \\(s^{2}_{2}\\) is the sample variance of the second sample. - \\(n_{1}\\) is the number of observations in the first sample. - \\(n_{2}\\) is the number of observations in the second sample.\nTwo-Sample Test – Standard Deviation of Differences\n\\(s_{d} = \\sqrt{\\frac{\\sum(d - \\bar{d})^2}{n - 1}}\\)\nwhere: - \\(\\bar{d}\\) is the mean of the difference between the paired or related observations. - \\(s_{d}\\) is the standard deviation of the differences between the paired or related observations. - \\(n\\) is the number of paired observations.\n\nTwo-Sample Test – Paired \\(t\\) Test\n\\(t = \\frac{\\bar{d}}{\\frac{s_{d}}{\\sqrt{n}}}\\)\nThis test reduces variation from the sampling distribution compared to independent samples but usually has fewer degrees of freedom (\\(n-1\\)).\nAn alternative for dependent samples is the Wilcoxon signed-rank test, which does not require the normality assumption.\nwhere: - \\(\\bar{d}\\) is the mean of the difference between the paired or related observations. - \\(s_{d}\\) is the standard deviation of the differences between the paired or related observations. - \\(n\\) is the number of paired observations.\nANOVA – Test Statistic for Comparing Two Variances\n\\(F = \\frac{s^{2}_{1}}{s^{2}_{2}}\\)\nwhere: - \\(s^{2}_{1}\\) is the sample variance of the first sample. - \\(s^{2}_{2}\\) is the sample variance of the second sample. - \\(F\\) represents the F distribution.\nIf the null hypothesis is true, the test statistic follows the \\(F\\) distribution with \\(n_{1} - 1\\) and \\(n_{2} - 1\\) degrees of freedom.\nANOVA – Critical Value for F Statistic\n\\(F = \\frac{k - 1}{n - k}\\)\nwhere: - \\(F\\) represents the F distribution. - \\(k\\) is the number of treatments. - \\(n\\) is the total number of observations.\nIn ANOVA, the critical value of the F statistic determines whether the null hypothesis \\(H_{0}\\) can be rejected.\nANOVA – Components\nThe sum of the squared differences between the following defines the ANOVA terms:\n\n\n\n\n\n\n\n\n(a) Each Observation\n(b) Overall Mean or Treatment Mean\n(c) ANOVA Term\n\n\n\n\nOverall mean\nEach observation\nTotal variation\n\n\nOverall mean\nEach treatment mean\nTreatment variation\n\n\nTreatment mean\nEach observation\nRandom variation\n\n\n\n\n\n———- page spacer ————————-\nAssumptions for ANOVA - The samples are from independent populations. - The population variances must be equal. - The samples are from normal populations.\nIf these assumptions are not met, the Kruskal-Wallisnonparametric test is recommended.\nANOVA Table (One-Way)\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMean Square\nF\n\n\n\n\nTreatments\nSST\n\\(k-1\\)\n\\(\\frac{SST}{(k-1)} = MST\\)\n\\(\\frac{MST}{MSE}\\)\n\n\nError\nSSE\n\\(n-k\\)\n\\(\\frac{SSE}{(n-k)} = MSE\\)\n\n\n\nTotal\nSS Total\n\\(n-1\\)\n\n\n\n\n\nwhere: - \\(n\\): Total number of observations. - \\(k\\): Number of treatments. - \\(SS\\): Sum of squares. - \\(SST\\): Sum of squares due to treatments, calculated as \\(\\text{SS total} - SSE\\) or \\(\\sum{(\\bar{x}_C - \\bar{x}_G)^2}\\), where \\(\\bar{x}_C\\) is the treatment mean and \\(\\bar{x}_G\\) is the overall mean. - \\(SSE\\): Sum of squares due to errors, calculated as \\(\\sum{(x - \\bar{x}_C)^2}\\), where \\(x\\) is each observation and \\(\\bar{x}_C\\) is the treatment mean. - \\(\\text{SS Total}\\): Total variation, calculated as \\(\\sum{(x - \\bar{x}_G)^2}\\), where \\(x\\) is each observation and \\(\\bar{x}_G\\) is the overall mean. - \\(MST\\): Mean square for treatments, an estimate of variance. - \\(MSE\\): Mean square for errors.\nANOVA: Confidence Interval for the Difference in Treatment Means\n\\((\\bar{x}_{1} - \\bar{x}_{2}) \\pm t\\sqrt{\\text{MSE}\\left(\\frac{1}{n_{1}} + \\frac{1}{n_{2}}\\right)}\\)\nwhere: - \\(\\bar{x}_{1}\\): Mean of the first sample. - \\(\\bar{x}_{2}\\): Mean of the second sample. - \\(t\\): t distribution with degrees of freedom equal to \\(n-k\\). - \\(MSE\\): Mean square error term from the ANOVA table \\(\\left[\\frac{SSE}{n-k}\\right]\\). - \\(n_{1}\\): Number of observations in the first sample. - \\(n_{2}\\): Number of observations in the second sample.\nANOVA: Two-Way – Sum of Squares Blocks\n\\(\\text{SSB} = k\\sum(\\bar{x}_{b} - \\bar{x}_{G})^{2}\\)\nwhere: - \\(k\\): Number of treatments. - \\(b\\): Number of blocks. - \\(\\bar{x}_{b}\\): Sample mean of block \\(b\\). - \\(\\bar{x}_{G}\\): Overall mean.\n———– page spacer ————\n**ANOVA: Two-Way – Table*\nANOVA Table (Two-Way)\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMean Square\nF\n\n\n\n\nTreatments\nSST\n\\(k-1\\)\n\\(\\frac{SST}{(k-1)} = MST\\)\n\\(\\frac{MST}{MSE}\\)\n\n\nBlocks\nSSB\n\\(b-1\\)\n\\(\\frac{SSB}{(b-1)} = MSB\\)\n\\(\\frac{MSB}{MSE}\\)\n\n\nError\nSSE\n\\((k-1)(b-1)\\)\n\\(\\frac{SSE}{(k-1)(b-1)} = MSE\\)\n\n\n\nTotal\nSS Total\n\\(n-1\\)\n\n\n\n\n\nwhere: - \\(n\\): Total number of observations. - \\(k\\): Number of treatments. - \\(SS\\): Sum of squares. - \\(SST\\): Sum of squares due to treatments, calculated as \\(\\sum{(\\bar{x}_C - \\bar{x}_G)^2}\\) or \\(\\text{SS Total} - SSE - SSB\\). - \\(SSB\\): Sum of squares blocks, calculated as \\(k\\sum{(\\bar{x}_{b} - \\bar{x}_{G})^2}\\), where \\(\\bar{x}_b\\) is the block mean and \\(\\bar{x}_G\\) is the overall mean. - \\(SSE\\): Sum of squares due to errors, calculated as \\(\\sum{(x - \\bar{x}_C)^2}\\), where \\(\\bar{x}_C\\) is the treatment mean. - \\(\\text{SS Total}\\): Total variation, calculated as \\(\\sum{(x - \\bar{x}_G)^2}\\), where \\(x\\) is each sample observation and \\(\\bar{x}_G\\) is the overall mean. - \\(MST\\): Mean square for treatments, an estimate of variance. - \\(MSB\\): Mean square for blocks. - \\(MSE\\): Mean square for errors.\nIllustration of Two-Way ANOVA\nThe following is an illustration of how the different variations relate:\n\n\n\nIllustration of a Two-Way ANOVA\n\n\n————– page spacer ————–\n**ANOVA: Two-Way with Interaction (Factors) – Table\nANOVA Table with Interaction (Factors)\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMean Square\nF\n\n\n\n\nFactor A\nSSA\n\\(k-1\\)\n\\(\\frac{SSA}{(k-1)} = MSA\\)\n\\(\\frac{MSA}{MSE}\\)\n\n\nFactor B\nSSB\n\\(b-1\\)\n\\(\\frac{SSB}{(b-1)} = MSB\\)\n\\(\\frac{MSB}{MSE}\\)\n\n\nInteraction\nSSI\n\\((k-1)(b-1)\\)\n\\(\\frac{SSI}{(k-1)(b-1)} = MSI\\)\n\\(\\frac{MSI}{MSE}\\)\n\n\nError\nSSE\n\\(n-kb\\)\n\\(\\frac{SSE}{(n-kb)} = MSE\\)\n\n\n\nTotal\nSS Total\n\\(n-1\\)\n\n\n\n\n\n————– page spacer ————-\n\n\nLinear Regression: Correlation Coefficient\n\\(r = \\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{(n-1)s_{x}s_{y}}\\)\nSubject A measure of the strength of the linear relationship between two variables, ranging from \\(-1\\) to \\(+1\\).\nNotes - The correlation coefficient is independent of scale if \\(\\sum(x-\\bar{x})(y-\\bar{y})\\) is divided by \\(s_{x}\\) and \\(s_{y}\\). - It becomes independent of sample size when divided by \\((n-1)\\). - Correlation between two independent variables is uncritical if \\(-0.70 &lt; r &lt; 0.70\\). For a more precise test, use the variance inflation factor (VIF).\nwhere: - \\(r\\): Correlation coefficient. - \\(x\\): Variable value of the x population. - \\(y\\): Variable value of the y population. - \\(\\bar{x}\\): Mean of variable values in the x population. - \\(\\bar{y}\\): Mean of variable values in the y population. - \\(n\\): Number of observations in the sample. - \\((n-1)\\): Degrees of freedom. - \\(s_{x}\\): Standard deviation of the x population. - \\(s_{y}\\): Standard deviation of the y population.\nLinear Regression: t Test for the Correlation Coefficient \\(r\\)\n\\(t = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\)\nSubject: Resolves the question about whether there could be zero correlation in the population from which the sample was selected.\nwhere - \\(r\\): Correlation coefficient. - \\(n\\): Number of observations in the sample.\n**Linear Regression Equation: General Form\n\\(\\hat{y} = a + bx\\)\nSubject An equation that expresses the linear relationship between two variables.\nwhere: - \\(\\hat{y}\\): Estimated value of the \\(y\\) variable for a selected \\(x\\) value. - \\(a\\): Y-intercept. It is the estimated value of \\(y\\) when \\(x = 0\\). - \\(b\\): Slope of the line, or the average change in \\(\\hat{y}\\) for each change of one unit (either increase or decrease) in the independent variable \\(x\\). - \\(x\\): Any value of the independent variable that is selected.\nLinear Regression: Slope of Regression Line\n\\(b = r\\left(\\frac{s_{y}}{s_{x}}\\right)\\)\nwhere: - \\(r\\): Correlation coefficient.\n\n\n\\(s_{y}\\): Standard deviation of \\(y\\) (the dependent variable).\n\\(s_{x}\\): Standard deviation of \\(x\\) (the independent variable).\n\nLinear Regression: Y-Intercept\n\\(a = \\bar{y} - b\\bar{x}\\)\nwhere: - \\(\\bar{y}\\): Mean of \\(y\\) (the dependent variable). - \\(\\bar{x}\\): Mean of \\(x\\) (the independent variable).\nLinear Regression: t Test for the Slope \\(b\\)\n\\(t = \\frac{b - 0}{s_{b}}\\)\nSubject: Conducts a test on whether the slope of the regression line is different from zero. In such a circumstance, we can reasonably conclude that the regression line adds to the predictive ability of the regression equation.\nwhere: - \\(b\\): Estimate of the regression line’s slope calculated from the sample information. - \\(s_{b}\\): Standard error of the slope estimate, also determined from sample information. - Degrees of freedom: \\(n-2\\).\nLinear Regression: Standard Error of Estimate\n\\(s_{y \\cdotp x} = \\sqrt{\\frac{\\sum(y-\\hat{y})^2}{n-2}} = \\sqrt{\\frac{\\text{SSE}}{n-2}}\\)\nSubject: A relative measure of a regression equation’s ability to predict.\nwhere: - \\(s_{y \\cdotp x}\\): Standard error of estimate with \\(y \\cdotp x\\) interpreted as the standard error of \\(y\\) for a given value of \\(x\\). It is analogous to the standard deviation, measuring dispersion around a mean. - \\(y\\): Observed value. - \\(\\hat{y}\\): Predicted value. - \\(\\sum(y-\\hat{y})^2\\): Sum of squares error (SSE) or residuals.\nLinear Regression: Coefficient of Determination\n\\(r^2 = \\frac{\\text{SSR}}{\\text{SS Total}} = \\frac{\\sum(\\hat{y}-\\bar{y})^2}{\\sum(y-\\bar{y})^2} = 1 - \\frac{\\text{SSE}}{\\text{SS Total}} = 1 - \\frac{\\sum(y-\\hat{y})^2}{\\sum(y-\\bar{y})^2}\\)\nSubject: The proportion of the total variation in the dependent variable \\(y\\) that is explained, or accounted for, by the variation in the independent variable \\(x\\).\nwhere: - SS Total: Total variation (sum of squares total). - SSR: Sum of squares regression. - SSE: Sum of squares errors or residuals.\n\n\n\n\nLinear Regression: Confidence Interval for the Mean of \\(y\\) Given \\(x\\)\n\\(\\hat{y} \\pm t s_{y \\cdotp x}\\sqrt{\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{\\sum(x-\\bar{x})^2}}\\)\nSubject: Applied when the regression equation is used to predict the mean value of \\(y\\) for a given value of \\(x\\).\nwhere: - \\(x\\): Given value. - \\(\\bar{x}\\): Sample mean. - \\(\\hat{y}\\): Predicted value. - \\(s_{y \\cdotp x}\\): Standard error of estimate, interpreted as the standard error of \\(y\\) for a given value of \\(x\\). Similar to the standard deviation, measuring dispersion around a mean.\nLinear Regression: Prediction Interval for \\(y\\) Given \\(x\\)\n\\(\\hat{y} \\pm t s_{y \\cdotp x}\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{\\sum(x-\\bar{x})^2}}\\)\nSubject: Applied when the regression equation is used to predict an individual value of \\(y\\) (\\(n = 1\\)) for a given value of \\(x\\). Refer to the confidence interval equation for comparison.\nwhere: - \\(x\\): Given value. - \\(\\bar{x}\\): Sample mean. - \\(\\hat{y}\\): Predicted value. - \\(s_{y \\cdotp x}\\): Standard error of estimate, interpreted as the standard error of \\(y\\) for a given value of \\(x\\). Similar to the standard deviation, measuring dispersion around a mean.\nMultiple Regression: General Equation\n\\(\\hat{y} = a + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} + ... + b_{k}x_{k}\\)\nSubject: Enhanced equation of the linear regression formula for more than one dependent variable.\nwhere: - \\(a\\): Intercept, the value of \\(\\hat{y}\\) when all the \\(x\\)’s are zero. - \\(b_{j}\\): The amount by which \\(\\hat{y}\\) changes when that particular \\(x_{j}\\) increases by one unit, with the values of all other independent variables held constant. - \\(k\\): Number of independent variables.\nMultiple Regression: Standard Error of Estimate\n\\(s_{y \\cdotp 123 \\cdotp k} = \\sqrt{\\frac{\\sum(y-\\hat{y})^2}{n-(k+1)}} = \\sqrt{\\frac{\\text{SSE}}{n-(k+1)}} = \\sqrt{\\text MSE}\\)\nSubject: A relative measure of a regression equation’s ability to predict for more than one independent variable.\nwhere: - \\(y\\): Actual observation. - \\(\\hat{y}\\): Estimated value computed from the regression equation.\n\n\n\\(n\\): Number of observations in the sample.\n\\(k\\): Number of independent variables.\n\\(SSE\\): Residual sum of squares from an ANOVA table, equal to \\(\\sum(y-\\hat{y})^2\\).\n\nMultiple Regression ANOVA – Table\nMultiple Regression ANOVA Table\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nRegression\nSSR\n\\(k\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(\\frac{MSR}{MSE}\\)\n\n\nResidual or error\nSSE\n\\(n-(k+1)\\)\n\\(MSE = \\frac{SSE}{n-(k+1)}\\)\n\n\n\nTotal\nSS Total\n\\(n-1\\)\n\n\n\n\n\nMultiple Regression: Coefficient of Multiple Determination\n\\(R^2 = \\frac{\\text{SSR}}{\\text{SS Total}}\\)\nSubject: The percent of variation in the dependent variable, \\(y\\), explained by the set of independent variables, \\(x_{1}\\), \\(x_{2}\\), \\(x_{3}\\), … \\(x_{k}\\).\nwhere: - SS Total: Total variation (sum of squares total). - SSR: Sum of squares regression.\nMultiple Regression: Adjusted Coefficient of Multiple Determination\n\\(R^2_{adj} = \\frac{\\frac{SSE}{n-(k+1)}}{\\frac{\\text{SS Total}}{n-1}}\\)\nSubject: The percent of variation in the dependent variable, \\(y\\), explained by the set of independent variables, \\(x_{1}\\), \\(x_{2}\\), \\(x_{3}\\), … \\(x_{k}\\).\nAs more independent variables are added to the multiple regression model, \\(R^2\\) tends to increase. If the number of variables, \\(k\\), and the sample size, \\(n\\), are equal, the coefficient of determination becomes 1. To avoid this trend, \\(R^2\\) is adjusted.\nwhere: - SS Total: Total variation (sum of squares total). - SSE: Sum of squares error or residual.\nMultiple Regression: Global Test\n\\(F = \\frac{\\frac{\\text{SSR}}{k}}{\\frac{\\text{SSE}}{n-(k+1)}}\\)\nSubject: The Global Test investigates whether it is possible that all the independent variables have zero regression coefficients. It compares the sum of squares regression per unit of sum of squares residuals. The higher the explained variances compared to the residual variances, the more positive the value of the F distribution.\nwhere: - SSR: Sum of squares regression.\n\n\n————- page spacer —————–\n\n\n\nSSE: Sum of squares error or residual.\n\\(n\\): Number of observations in the sample.\n\\(k\\): Number of independent variables.\n\nMultiple Regression: t Test Individual Coefficients \\(b\\)\n\\(t = \\frac{b_{j} - 0}{s_{b_{j}}}\\)\nSubject: Tests the independent variables individually to determine whether the regression coefficients differ from zero. If a regression coefficient is likely to be zero, it does not contribute to the regression equation’s ability to predict.\nwhere: - \\(b_{j}\\): Any one of the regression coefficients. - \\(s_{b_{j}}\\): Standard error of the slope estimate, determined from sample information.\nMultiple Regression: Variance Inflation Factor\n\\(VIF = \\frac{1}{1 - R^2_{j}}\\)\nSubject: A VIF greater than 10 is considered unsatisfactory, indicating that the independent variable should be removed from the analysis.\nwhere: - \\(R^2_{j}\\): Coefficient of determination.\nTest of Hypothesis: One Proportion\n\\(z = \\frac{p - \\pi}{\\sqrt{\\frac{\\pi(1 - \\pi)}{n}}}\\)\nwhere: - \\(\\pi\\): Population proportion. - \\(p\\): Sample proportion. - \\(n\\): Sample size.\nTest of Hypothesis: Pooled Proportion\n\\(p_c = \\frac{x_1 + x_2}{n_1 + n_2}\\)\nwhere: - \\(p_{c}\\): Pooled proportion possessing the trait in the combined samples (pooled estimate of the population proportion). - \\(x_1\\): Number possessing the trait in the first sample. - \\(x_2\\): Number possessing the trait in the second sample. - \\(n_{1}\\): Number of observations in the first sample. - \\(n_{2}\\): Number of observations in the second sample.\nTest of Hypothesis: Two-Sample Test of Proportions\n\\(z = \\frac{p_{1} - p_{2}}{\\sqrt{\\frac{p_{c}(1 - p_{c})}{n_{1}} + \\frac{p_{c}(1 - p_{c})}{n_{2}}}}\\)\nwhere: - \\(n_{1}\\): Number of observations in the first sample.\n\n\n\\(n_{2}\\): Number of observations in the second sample.\n\\(p_{1}\\): Proportion in the first sample possessing the trait.\n\\(p_{2}\\): Proportion in the second sample possessing the trait.\n\\(p_{c}\\): Pooled proportion possessing the trait in the combined samples (pooled estimate of the population proportion).\n\nChi-Square Test Statistic\n\\(\\chi^2 = \\sum\\left[\\frac{(f_o - f_e)^2}{f_e}\\right]\\)\nDegrees of Freedom \\(k - 1\\)\nwhere: - \\(k\\): Number of categories. - \\(f_o\\): Observed frequency in a particular category. - \\(f_e\\): Expected frequency in a particular category.\nExpected Frequency\n\\(f_e = \\frac{\\text{(row total) \\text{(column total)}}}{\\text{(grand total)}}\\)\nwhere: - \\(f_e\\): Expected frequency in a particular category.\nSign Test: \\(n &gt; 10\\)\n\\(z = \\frac{(x \\pm 0.50) - \\mu}{\\sigma}\\)\nwhere: - \\(z\\): Standard value. - \\(\\pm 0.50\\): Continuity correction factor. - \\(x\\): Number of plus (\\(+\\)) or minus (\\(-\\)) signs. - \\(\\mu\\): Population mean. - \\(\\sigma\\): Population standard deviation.\nSign Test: \\(n &gt; 10\\), + Signs More Than \\(n/2\\)\n\\(z = \\frac{(x - 0.50) - \\mu}{\\sigma} = \\frac{(x - 0.50) - 0.50n}{0.50\\sqrt{n}}\\)\nwhere:\n\n\n—————– page spacer ———————-\n\n\n\n\\(z\\): Standard value.\n\\(\\pm 0.50\\): Continuity correction factor.\n\\(x\\): Number of plus (\\(+\\)) or minus (\\(-\\)) signs.\n\\(n\\): Sample size.\n\\(\\mu\\): Population mean.\n\\(\\sigma\\): Population standard deviation.\n\nSign Test: \\(n &gt; 10\\), + Signs LessThan \\(n/2\\)\n\\(z = \\frac{(x + 0.50) - \\mu}{\\sigma} = \\frac{(x + 0.50) - 0.50n}{0.50\\sqrt{n}}\\)\nwhere: - \\(z\\): Standard value. - \\(\\pm 0.50\\): Continuity correction factor. - \\(x\\): Number of plus (\\(+\\)) or minus (\\(-\\)) signs. - \\(n\\): Sample size. - \\(\\mu\\): Population mean. - \\(\\sigma\\): Population standard deviation.\nWilcoxon Rank-Sum Test\n\\(z = \\frac{W - \\frac{n_1(n_1 + n_2 + 1)}{2}}{\\sqrt{\\frac{n_1n_2(n_1 + n_2 + 1)}{12}}}\\)\nSubject: This test is specifically designed to determine whether two independent samples came from equivalent populations.\nNote This test is an alternative to the Two-Sample t test but does notrequire that the two populations follow the normal distribution or have equal population variances.\nwhere: - \\(n_1\\): Number of observations of the first population. - \\(n_2\\): Number of observations of the second population. - \\(W\\): Sum of the ranks from the first population.\nKruskal-Wallis Test\n\\(H = \\frac{12}{n(n + 1)}\\left[\\frac{(\\sum R_1)^2}{n_1} + \\frac{(\\sum R_2)^2}{n_2} + ... + \\frac{(\\sum R_k)^2}{n_k}\\right] - 3(n+1)\\)\nNote For the Kruskal-Wallis test to be applied, the samples selected from the populations must be independent. If the following prerequisites are met, an ANOVA analysis can be applied instead: - The samples are from independent populations. - The population variances must be equal. - The samples are from normal populations.\nDegrees of Freedom \\(k-1\\), where \\(k\\) is the number of populations.\nwhere: - \\(\\sum R_1, \\sum R_2, ..., \\sum R_k\\): Sums of the ranks of samples 1, 2, …, \\(k\\). - \\(n_1, n_2, ..., n_k\\): Sizes of samples 1, 2, …, \\(k\\). - \\(n\\): Combined number of observations for all samples.\nSpearman’s Coefficient of Rank Correlation\n\\(r_s = 1 - \\frac{6\\sum d^2}{n(n^2 - 1)}\\)\nwhere: - \\(r_s\\): Spearman’s coefficient of rank correlation. - \\(d\\): Difference between the ranks for each pair. - \\(n\\): Number of paired observations.\nHypothesis Test: Rank Correlation\n\\(t = r_s\\sqrt{\\frac{n - 2}{1 - r_s^2}}\\)\nwhere:\n\n\n\\(r_s\\): Spearman’s coefficient of rank correlation.\n\nIndex Numbers: Simple Index\n\\(P = \\frac{p_t}{p_0} \\times 100\\)\nwhere: - \\(p_0\\): Base-period price. - \\(p_t\\): Given period price.\nIndex Numbers: Simple Average of the Price Relatives\n\\(P = \\frac{\\sum P_i}{n}\\)\nNote - Advantage: Simple price indices are not dependent on the unit of measure of the item quantified. - Disadvantage: Simple price indices do not account for the relative importance of the items included.\nThe lack of consideration for the relative importance of items is addressed by the Laspeyres price index and the Paasche price index.\nwhere: - \\(P_i\\): Simple index for each of the items. - \\(n\\): Number of items.\nIndex Numbers: Simple Aggregate Index\n\\(P = \\frac{\\sum p_t}{\\sum p_0} \\times 100\\)\nNote Since the aggregate index is influenced by the unit of measure, it is not used frequently.\nwhere: - \\(p_0\\): Base-period price. - \\(p_t\\): Given period price.\nIndex Numbers: Laspeyres Price Index\n\\(P = \\frac{\\sum p_t q_0}{\\sum p_0 q_0} \\times 100\\)\nNote The Laspeyres price index assumes that the base-period quantities still have a significant bearing on the current price index and are realistic. This contrasts with the assumption of the Paasche price index.\nwhere: - \\(P\\): Price index. - \\(p_t\\): Current price. - \\(p_0\\): Price in the base period. - \\(q_0\\): Quantity used in the base period.\nIndex Numbers: Paasche Price Index\n\\(P = \\frac{\\sum p_t q_t}{\\sum p_0 q_t} \\times 100\\)\nNote The Paasche price index assumes current period quantity levels as the base to account for changed preferences in consumed quantities. This contrasts with the assumption of the Laspeyres price index.\nwhere:\n\n\n————— page spacer ———————\n\n\n\n\\(P\\): Price index.\n\\(p_t\\): Current price.\n\\(p_0\\): Price in the base period.\n\\(q_t\\): Quantity used in the current period.\n\nIndex Numbers: Fisher’s Ideal Index\n\\(\\text{Fisher's Ideal Index} = \\sqrt{\\text{(Laspeyres index)}\\text{(Paasche index)}}\\)\nNote Fisher’s Ideal Index is a geometric mean of the Laspeyres and Paasche price indices.\nIndex Numbers: Value Index\n\\(V = \\frac{\\sum p_t q_t}{\\sum p_0 q_0} \\times 100\\)\nwhere: - \\(P\\): Price index. - \\(p_t\\): Current price. - \\(p_0\\): Price in the base period. - \\(q_0\\): Quantity used in the base period.\nIndex Numbers: Real Income\n\\(\\text{Real income} = \\frac{\\text{Money income}}{\\text{CPI}} \\times 100\\)\nwhere: - \\(CPI\\): Consumer price index.\nIndex Numbers: Index as a Deflator\n\\(\\text{Deflated sales} = \\frac{\\text{Actual sales}}{\\text{An appropriate index}} \\times 100\\)\nIndex Numbers: Index for Purchasing Power\n\\(\\text{Purchasing power of dollar} = \\frac{\\text{\\$1}}{\\text{CPI}} \\times 100\\)\nTime Series & Forecasting: Linear Trend Equation\n\\(\\hat{y} = a + bt\\)\nwhere: - \\(\\hat{y}\\): Projected value of the \\(y\\) variable for a selected value of \\(t\\). - \\(a\\): Y-intercept, the estimated value of \\(y\\) when \\(t = 0\\). - \\(b\\): Slope of the line, or the average change in \\(\\hat{y}\\) for each increase of one unit in \\(t\\). - \\(t\\): Any value of time that is selected.\nTime Series & Forecasting: Log Trend Equation\n\\(\\log \\hat{y} = \\log a + \\log b(t)\\)\n\nTime Series & Forecasting: Correction Factor for Adjusting Quarterly Means \\(\\text{Correction factor} = \\frac{4.00}{\\text{Total of four means}}\\)\nTime Series & Forecasting: Durbin-Watson Statistic\n\\(d = \\frac{\\sum_{t = 2}^{n} (e_t - e_{t-1})^2}{\\sum_{t = 1}^n(e_t)^2}\\)\n\n\nLaTeX Template: Two Column Colour Article\nSource: http://www.howtotex.com/ Feel free to distribute this template, but please keep the referal to howtotex.com. Date: Feb 2011\n\n\n\n\n Back to top"
  },
  {
    "objectID": "extra.html",
    "href": "extra.html",
    "title": "extra",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "graphicalsummaries.html",
    "href": "graphicalsummaries.html",
    "title": "Graphical Summaries",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Graphical Summaries"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "inference/hypothesis.html",
    "href": "inference/hypothesis.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "numericalsummaries.html",
    "href": "numericalsummaries.html",
    "title": "Numerical Summaries",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Numerical Summaries"
    ]
  }
]