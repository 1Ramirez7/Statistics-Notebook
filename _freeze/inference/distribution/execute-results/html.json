{
  "hash": "cff186e6bcd69ef90dd3129d7af1681c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distributions\"\nsubtitle: \"Population vs Sampling\"\nformat: \n  html:\n    error: false\n    message: false\n    warning: false\n    toc: true\n    code-fold: true\n    css: ../styles.css\n    math: katex  \n---\n\n\n\n**Sampling Distributions**\n\nKey points about modality and skewness in sampling distributions:\n\nModality:\n\n-   Unimodal: A sampling distribution with only one prominent peak.\n\n-   Bimodal: A sampling distribution with two distinct peaks.\n\n-   Multimodal: A sampling distribution with more than two peaks.\n\n-   Uniform: In a uniform distribution, every value within a specified range has an equal probability of occurring, creating a flat horizontal line when visualized on a graph.\n\nSkewness:\n\n-   Positive Skewness (Right Skewed): The tail of the distribution extends further to the right side, meaning there are a few very high values compared to the majority of lower values.\n\n-   Negative Skewness (Left Skewed): The tail of the distribution extends further to the left side, indicating a few very low values compared to most higher values.\n\n-   Zero Skewness (Symmetrical): The distribution is balanced with the left and right sides mirroring each other.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Unimodal\nggplot(data.frame(x = rnorm(1000)), aes(x)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  ggtitle(\"Unimodal Distribution\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Bimodal\nggplot(data.frame(x = c(rnorm(500, -2), rnorm(500, 2))), aes(x)) +\n  geom_density(fill = \"green\", alpha = 0.5) +\n  ggtitle(\"Bimodal Distribution\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Multimodal\nggplot(data.frame(x = c(rnorm(300, -4), rnorm(300, 0), rnorm(300, 4))), aes(x)) +\n  geom_density(fill = \"purple\", alpha = 0.5) +\n  ggtitle(\"Multimodal Distribution\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Uniform\nggplot(data.frame(x = runif(1000, -5, 5)), aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"orange\", color = \"black\", alpha = 0.5) +\n  ggtitle(\"Uniform Distribution\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Positive Skew\nggplot(data.frame(x = rexp(1000, rate = 1)), aes(x)) +\n  geom_density(fill = \"red\", alpha = 0.5) +\n  ggtitle(\"Positive Skew (Right Skewed)\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Negative Skew\nggplot(data.frame(x = -rexp(1000, rate = 1)), aes(x)) +\n  geom_density(fill = \"cyan\", alpha = 0.5) +\n  ggtitle(\"Negative Skew (Left Skewed)\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Symmetrical\nggplot(data.frame(x = rnorm(1000)), aes(x)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  ggtitle(\"Zero Skewness (Symmetrical)\")\n```\n\n::: {.cell-output-display}\n![](distribution_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\n\nKey Differences Between General and Sampling Distributions\n\n------------------------------------------------------------------------\n\n| **Aspect** | **General Distributions** | **Sampling Distributions** |\n|----------------|---------------------------|------------------------------|\n| **Focus** | Distribution of raw data or random variables | Distribution of a statistic (e.g., sample mean, proportion) |\n| **Population or Sample** | Describes population or a single sample | Derived from repeated sampling of a population |\n| **Examples** | Normal, Uniform, Exponential | Sampling distribution of sample mean, proportion, etc. |\n| **Derived From** | Directly observed data | Repeatedly calculated from samples |\n| **Shape** | Depends on the data (e.g., normal, skewed) | Depends on sample size and population (Central Limit Theorem often applies) |\n| **Role in Statistics** | Describe the data or population characteristics | Used for inferential statistics (e.g., confidence intervals, hypothesis testing) |\n\n**Example 2.12 Student Math Performance econometrics**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(wage1, package='wooldridge')\n\n# Estimate log-level model\nlm( log(wage) ~ educ, data=wage1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(wage) ~ educ, data = wage1)\n\nCoefficients:\n(Intercept)         educ  \n    0.58377      0.08274  \n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**Frequency Distributions for categorical data**\n\n A frequency distribution is a table used to organize data. The left column \n(called classes or groups) includes all possible responses on a variable being \nstudied. The right column is a list of the frequencies, or number of observa\ntions, for each class. (see png 27813)(book278 1.3)\n\n\n**Relative frequency Distribution** \n\nis obtained by dividing \neach frequency by the number of observations and multiplying the resulting \nproportion by 100%.  (see png 27813)(book278 1.3)\n\n\n\n\n\n**Frequency Distributions for numerical data**\n\nBook278 1.5\n\nSimilar to a frequency distribution for categorical data (Section1.3), a frequency distribution for numerical data is a table that summarizes data by listing the classes in the left column and the number of observations in each class in the right column. However, the classes, or intervals, for a frequency distribution of numerical data are not as easily identifiable. Determining the classes of a frequency distribution for numerical data requires an swers to certain questions: How many classes should be used? How wide should each class be?\n\n**Cumulative Frequency Distribution** \n\n A cumulative frequency distribution contains the total number of observations \nwhose values are less than the upper limit for each class. We construct a  \ncumulative frequency distribution by adding the frequencies of all frequency \ndistribution classes up to and including the present class.  (see png 27815) (ECON278 1.5)\n\n\n**Relative Cumulative Frequency Distribution**\n\nIn a relative cumula\ntive frequency distribution, cumulative frequencies can be expressed as cu\nmulative proportions or percents. (ECON278 1.5) (see png 27815)\n\n\n::::\n:::: {.column width=\"50%\"}\n\n![27813](../images/27813.png)\n\n**Construction of a Frequency Distribution**\n![](../images/278131.png)\n\n![27815](../images/27815.png)\n\n::::\n:::\n\n\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**Shape of a Distribution**\n\nWe can describe graphically the shape of the distribution by a histogram. That is, we can \nvisually determine whether data are evenly spread from its middle or center. Sometimes \nthe center of the data divides a graph of the distribution into two “mirror images,” so that the portion on one side of the middle is nearly identical to the portion on the other \nside. Graphs that have this shape are symmetric; those without this shape are asymmetric, \nor skewed. \n\n**Symmetry**\n\nThe shape of a distribution is said to be symmetric if the observations are bal\nanced, or approximately evenly distributed, about its center.\n\n**Skewness**\n\n A distribution is skewed, or asymmetric, if the observations are not sym\nmetrically distributed on either side of the center. A skewed-right distribution \n(sometimes called positively skewed) has a tail that extends farther to the \nright. A skewed-left distribution (sometimes called negatively skewed) has a \ntail that extends farther to the left.\n\n\nFigure 278151, Figure 1.15(b), and Figure 1.15(c) illustrate a histogram for a continu\nous numerical unimodal variable with a symmetric distribution, a skewed-right distribu\ntion and a skewed-left distribution, respectively.\n\n**Box-and-Whisker Plot**\n\nA box-and-whisker plot is a graph that describes the shape of a distribution \nin terms of the five-number summary: the minimum value, first quartile (25th \npercentile), the median, the third quartile (75th percentile), and the maximum \nvalue. The inner box shows the numbers that span the range from the first to \nthe third quartile. A line is drawn through the box at the median. There are \ntwo “whiskers.” One whisker is the line from the 25th percentile to the mini\nmum value; the other whisker is the line from the 75th percentile to the maxi\nmum value. Using Minitab, we see in Figure 27822 the shapes of the distribution of sales for these \nfour locations.(ECON278 2.2) (see png 27822)\n\n\n::::\n:::: {.column width=\"50%\"}\n\n![278151](../images/278151.png)\n![ 27822](../images/27822.png)\n\n::::\n:::\n\n\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**Probability Distribution Function**\n\nThe probability distribution function, $P(x)$, of a discrete random variable $X$ \nrepresents the probability that $X$ takes the value $x$, as a function of $x$. That is,\n$$P(x) = P(X = x), \\text{for all values of } x$$.\nWe use the term probability distribution to represent probability distribution \nfunctions in this book, following the common practice.\n\n**Required Properties of Probability Distribution for Discrete Random Variables**\n\nLet $X$ be a discrete random variable with probability distribution $P(x)$. Then,\n\n1. $0 \\leq P(x) \\leq 1$ for any value $x$, and  \n2. the individual probabilities sum to 1, that is,  \n   $$\\sum_{x} P(x) = 1 \\tag{4.1}$$  \n   where the notation indicates summation over all possible values of $x$.\n   \n   \n**Cumulative Probability distribution**\n\nThe cumulative probability distribution, $F(x_0)$, of a random variable $X$, represents the probability that $X$ does not exceed the value $x_0$, as a function of $x_0$.  \nThat is,  \n$$F(x_0) = P(X \\leq x_0) \\tag{4.2}$$  \nwhere the function is evaluated at all values of $x_0$.\n\n\n\n\n::::\n:::: {.column width=\"50%\"}\n\n![ ](../images/27842.png)\n**Derived Relationship Between Probability Distribution and Cumulative Probability Distribution**\n\nLet $X$ be a random variable with probability distribution $P(x)$ and cumulative probability distribution $F(x_0)$. Then we can show that  \n$$F(x_0) = \\sum_{x \\leq x_0} P(x) \\tag{4.3}$$  \nwhere the notation implies that summation is over all possible values of $x$ that are less than or equal to $x_0$.\n\n**Derived Properties of Cumulative Probability Distributions for Discrete Random Variables**\n\nLet $X$ be a discrete random variable with cumulative probability distribution $F(x_0)$. Then we can show that:\n\n1. $0 \\leq F(x_0) \\leq 1$ for every number $x_0$, and  \n2. if $x_0$ and $x_1$ are two numbers with $x_0 \\leq x_1$, then $F(x_0) \\leq F(x_1)$.\n\n\n::::\n:::\n\n\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**The Binomial Distribution**\n\nSuppose that a random experiment can result in two possible mutually exclusive and collectively exhaustive outcomes, “success” and “failure,” and that $P$ is the probability of a success in a single trial. If $n$ independent trials are carried out, the distribution of the number of resulting successes, $x$, is called the binomial distribution. Its probability distribution function for the binomial random variable $X = x$ is as follows:  \n\n$P(\\text{$x$ successes in $n$ independent trials})$\n\n$$P(x) = \\frac{n!}{x!(n-x)!} P^x (1-P)^{n-x} \\tag{4.18}$$\n$$\\text{for } x = 0, 1, 2, \\ldots, n.$$\n\n\n::::\n:::: {.column width=\"50%\"}\n\n**Mean and Variance of a Binomial Probability Distribution**\n\nLet $X$ be the number of successes in $n$ independent trials, each with a probability of success $P$. Then $X$ follows a binomial distribution with mean  \n$$\\mu = E[X] = nP \\tag{4.19}$$  \nand variance  \n$$\\sigma_X^2 = E[(X - \\mu_X)^2] = nP(1-P) \\tag{4.20}$$  \n\nThe derivation of the mean and variance of the binomial is shown in Section 4 of the chapter appendix.\n\n\n::::\n:::\n\n\n\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**Sampling Distributions**\n\n Consider a random sample selected from a population that is used to make an \ninference about some population characteristic, such as the population mean, $\\mu$, using a sample statistic, such as the sample mean, $\\bar{x}$. We realize that every \nsample has different observed values and, hence, different sample means. The \nsampling distribution of the sample mean is the probability distribution of the \nsample means obtained from all possible samples of the same number of ob\nservations drawn from the population. Using the sampling distribution we can \nmake an inference about the population mean. (see 27861)\n\n![278611](../images/278611.png)\n\n\n::::\n:::: {.column width=\"50%\"}\n\n![27861](../images/27861.png)\n\n**conlcusion to png 27861 and start of png 278611**\n\nWe see that, although the number of years of experience for the six workers ranges \nfrom 2 to 8, the possible values of the sample mean have a range from only 3.0 to 7.5. In \naddition, more of the values lie in the central portion of the range.\n Table 6.3 presents similar results for a sample size of $n=5$, and Figure 6.2 presents \nthe graph for the sampling distribution. Notice that the means are concentrated over a \nnarrower range. These sample means are all closer to the population mean, $\\mu =5.5$. We \nwill always find this to be true—the sampling distribution becomes concentrated closer \nto the population mean as the sample size increases. This important result provides an \nimportant foundation for statistical inference. In the following sections and chapters, we \nbuild a set of rigorous analysis tools on this foundation. (see 278611) (ECON278 6.1)\n\n\n\n\n::::\n:::\n\n\n---\n\n::: {.columns}\n:::: {.column width=\"50%\"}\n\n**Standard Normal Distribution for the Sample Means**\n\nWhenever the sampling distribution of the sample means is a normal distribution, we can compute a standardized normal random variable, $Z$, that has a mean of 0 and a variance of 1:  \n\n$$Z = \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{x}}} = \\frac{X - \\mu}{\\sigma / \\sqrt{n}} \\tag{6.1}$$\n**Results for the Sampling Distribution of the Sample Means**\n\nLet $\\bar{X}$ denote the sample mean of a random sample of $n$ observations from a population with mean $\\mu_X$ and variance $\\sigma^2$.  \n\n1. The sampling distribution of $\\bar{X}$ has mean:  \n   $$E[\\bar{X}] = \\mu \\tag{6.2}$$\n\n2. The sampling distribution of $\\bar{X}$ has standard deviation:  \n   $$\\sigma_X = \\frac{\\sigma}{\\sqrt{n}} \\tag{6.3}$$  \n   This is called the standard error of $\\bar{X}$.  \n\n3. If the sample size, $n$, is not small compared to the population size, $N$, then the standard error of $\\bar{X}$ is as follows:  \n   $$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\sqrt{\\frac{N - n}{N - 1}} \\tag{6.4}$$  \n\n4. If the parent population distribution is normal and, thus, the sampling distribution of the sample means is normal, then the random variable  \n   $$Z = \\frac{X - \\mu}{\\sigma_{\\bar{x}}} \\tag{6.5}$$  \n   has a standard normal distribution with a mean of 0 and a variance of 1.\n\n\n::::\n:::: {.column width=\"50%\"}\n\nFigure 6.3(27862) shows the sampling distribution of the sample means for sample sizes \nn = 25 and n = 100 from a normal distribution. Each distribution is centered on the \nmean, but as the sample size increases, the distribution becomes concentrated more \nclosely around the population mean because the standard error of the sample mean de\ncreases as the sample size increases. Thus, the probability that a sample mean is a fixed \ndistance from the population mean decreases with increased sample size. (ECON278 2.2)\n\n![27862](../images/27862.png)\n\n\n\n::::\n:::\n",
    "supporting": [
      "distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}