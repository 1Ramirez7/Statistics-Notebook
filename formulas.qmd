---
title: "Statistics Formula Collection"
subtitle: ""
format: 
  html:
    page-layout: full
    error: false
    message: false
    warning: false
    toc: false
    code-fold: true
    math: katex  katex, mathjax, none 
    css: style.css
---


::: {.columns}
:::: {.column width="50%"}

**Frequency distributions**

Constructing frequency distributions where $k$ is the smallest number of classes and $n$ the number of observations:

1. Decide on the number of classes $k$ where $2^k > n$.
2. Determine the class interval $i$ by $i \geq \frac{\text{max value - min value}}{k}$.
3. Set individual class limits.
4. Tally the values into the classes.
5. Count the number of items in each class.



**Sample mean**

$\bar{x} = \frac{\sum x}{n}$

where:

- $\bar{x}$ denotes the sample mean.

- $n$ denotes the number of values in the sample.

- $x$ denotes any value.


**Median**

The midpoint of the values after they have been ordered from the minimum to the maximum values. The data must be at least an ordinal level of measurement.

::::
:::: {.column width="50%"}

**Mode**

The value of the observation that appears most frequently. It is especially useful in summarizing ordinal level data.

**Weighted mean**

$\bar{x}_{w} = \frac{w_1 x_1 + w_2 x_2 + \cdots + w_n x_n}{w_1 + w_2 + \cdots + w_n} = \frac{\sum\limits_{i=1}^n w_i x_i}{\sum\limits_{i=1}^n w_i}$

where:

- $\bar{x}_{w}$ denotes the weighted mean.

- $w$ denotes the corresponding weight.


**Geometric mean**

Geometric mean = $\sqrt[n]{(x_{1})(x_{2}) \cdots (x_{n})}$

Useful in finding the *average change* of percentages, ratios, indices, or growth rates over time. The geometric mean will always be less than or equal to (never more than) the arithmetic mean. All data values must be *positive*. It is applied in Fisher's Ideal Index.

**Geometric mean for a rate increase over time**

GM for a rate increase = $\sqrt[n]{\frac{\text{value at end of period}}{\text{value at start of period}}} - 1$

Used to find an average percentage *increase* over time.


**Range**

Range = maximum value - minimum value

**Population variance**

$\sigma^2 = \frac{\sum (x - \mu)^2}{N}$

The population variance $\sigma^2$ mitigates the dilemma of a single sample. In one sample, the deviation between an observed value $x$ and the population mean $\mu$ might

::::
:::



::: {.columns}
:::: {.column width="50%"}

differ greatly, while in another sample the deviation might differ significantly again. $\sigma^2$ provides a measure of the average variance accounting for all samples for one unit of the population.

where:

- $\sigma^2$ is the population variance.

- $x$ is the value of a particular observation in the population.

- $\mu$ is the arithmetic mean of the population.

- $N$ is the number of observations in the population.



**Population standard deviation**

$\sigma = \sqrt{\frac{\sum (x - \mu)^2}{N}}$

The population standard deviation $\sigma$ mitigates the dilemma of a single sample (refer to population variance). In one sample, the deviation between an observed value $x$ and the population mean $\mu$ might differ greatly, while in another sample, the deviation might differ significantly again. $\sigma$ provides a measure of the average deviation accounting for all samples for one unit of the population, in the same unit of measure as the sample.

where:

- $\sigma$ is the population standard deviation.

- $x$ is the value of a particular observation in the population.

- $\mu$ is the arithmetic mean of the population.

- $N$ is the number of observations in the population.

By taking the square root of the variance, the deviation is now in the same unit of measurement as the original data.



**Sample variance**

$s^2 = \frac{\sum (x - \bar{x})^2}{n - 1}$

where:

- $s^2$ is the sample variance.

- $x$ is the value of each observation in the sample.

- $\bar{x}$ is the mean of the sample.

- $n$ is the number of observations in the sample.

- The denominator $(n - 1)$ corrects its tendency for underestimation.

**Sample standard deviation**

$s = \sqrt{\frac{\sum (x - \bar{x})^2}{n - 1}}$

where:

- $s$ is the sample standard deviation.

- $x$ is the value of each observation in the sample.

- $\bar{x}$ is the mean of the sample.

- $n$ is the number of observations in the sample.

- By taking the square root of the variance, the deviation is now in the same unit of measurement as the original data.

::::
:::: {.column width="50%"}

**Chebyshev's Theorem**

For any set of observations (sample or population), the proportion of the values that lie within *k* standard deviations of the mean is at least $1 - \frac{1}{k^2}$, where *k* is any value greater than 1.

**Arithmetic mean of grouped data**

$\bar{x} = \frac{\sum{fM}}{n}$

where:

- $\bar{x}$ is the sample mean.

- $M$ is the midpoint of each class.

- $f$ is the frequency in each class.

- $n$ is the total number of frequencies.



**Standard deviation of grouped data**

$s = \sqrt{\frac{\sum f(M - \bar{x})^2}{n - 1}}$

where:

- $s$ is the sample standard deviation.

- $M$ is the midpoint of the class.

- $f$ is the class frequency.

- $\bar{x}$ is the mean of the sample.

- $n$ is the number of observations in the sample.

**Location of percentile**

$L_{p} = (n + 1)\frac{P}{100}$

where:

- $P$ is the percentile.

- $n$ is the number of observations.

- $L_{p}$ is the location of the percentile.


**Coefficient of Variation**

$\text{CV} = \frac{s}{\bar{x}}(100)$

Note: Multiplying by 100 converts the decimal to a percent.

where:

- $s$ is the sample standard deviation.

- $\bar{x}$ is the sample mean.

**Pearson's coefficient of skewness**

$\text{sk} = \frac{3(\bar{x} - \text{median})}{s}$

where:

- $s$ is the sample standard deviation.

- $\bar{x}$ is the sample mean.

::::
:::


-------------------- page spacer -------------------------

::: {.columns}
:::: {.column width="50%"}

**Software coefficient of skewness**

$\text{sk} = \frac{n}{(n - 1)(n - 2)}\left[\sum\left(\frac{x - \bar{x}}{s}\right)^{3}\right]$

where:

- $n$ is the number of observations in the sample.

- $s$ is the sample standard deviation.

- $\bar{x}$ is the sample mean.

**Classical probability**

$\text{Probability of an event} = \frac{\text{Number of favorable outcomes}}{\text{Total number of possible outcomes}}$




**Empirical probability**

$\text{Probability of an event} = \frac{\text{Number of times event occurred in the past}}{\text{Total number of observations}}$

**Special Rule of Addition**

$\text{P(A or B)} = P(A) + P(B)$

Events must be *mutually exclusive*.



**Complement Rule**

$P(A) = 1 - P(\sim A)$

Events $A$ and $\sim A$ are mutually exclusive and collectively exhaustive.

**General Rule of Addition**

$\text{P(A or B)} = P(A) + P(B) - P(A \text{ and } B)$

For events that are *not* mutually exclusive.

**Special Rule of Multiplication**

$\text{P(A and B)} = P(A)P(B)$

Requires that two events are *independent*, meaning the occurrence of one event has no effect on the probability of the occurrence of the other event.


**General Rule of Multiplication**

$\text{P(A and B)} = \text{P(A)}P(B \vert A)$

where:
- $P(B \vert A)$ is the probability that $B$ will occur given that $A$ has already occurred (conditional probability).
- The two events are *not independent*.

**Bayes' Theorem**

$P(A_{i} \vert B) = \frac{P(A_{i})P(B \vert A_{i})}{P(A_{1})P(B \vert A_{1}) + P(A_{2})P(B \vert A_{2})}$

Events $A_{1}$ and $A_{2}$ are mutually exclusive and collectively exhaustive, and $A_{i}$ refers to either event $A_{1}$ or $A_{2}$.
::::
:::: {.column width="50%"}


**Multiplication Formula**

$\text{Total number of arrangements} = (m)(n)$



**Permutation**

${}_nP_{r} = \frac{n!}{(n - r)!}$

Any arrangement of *r* objects selected from a single group of *n* possible objects.

**Combination Formula**

${}_nC_{r} = \frac{n!}{r!(n - r)!}$

The order of the selected objects is *not important*.

**Mean of a Probability Distribution**

$\mu = \Sigma \left[ xP(x) \right]$

where:
- $\mu$ is the population mean.
- $P(x)$ is the probability.
- $x$ is a particular value.



**Variance of a Probability Distribution**

$\sigma^{2} = \Sigma\left[ (x - \mu)^{2}P(x) \right]$

where:
- $\mu$ is the mean.
- $P(x)$ is the probability.
- $x$ is a particular value.

**Binomial Probability Distribution (with replacement)**

$P(x) = {}_nC_{x}\pi^{x}(1 - \pi)^{n - x}$

where:
- $C$ denotes a combination.
- $n$ is the number of trials.
- $x$ is the number of successes.
- $\pi$ is the probability of a success on each trial.

**Mean of a Binomial Distribution (with replacement)**

$\mu = n\pi$

where:
- $\mu$ is the probability mean.
- $n$ is the number of trials.
- $\pi$ is the probability of a success on each trial.

**Variance of a Binomial Distribution (with replacement)**

$\sigma^{2} = n\pi(1 - \pi)$

where:
- $n$ is the total number of trials.

::::
:::



------- page spacer --------------------

::: {.columns}
:::: {.column width="50%"}

- $\pi$ is the probability of a success on each trial.

**Hypergeometric Distribution (without replacement)**

$P(x) = \frac{({}_SC_{x})(C^{N-S}_{n-x})}{{}_NC_{n}}$

where:
- $N$ is the size of the population.
- $S$ is the number of successes in the population.
- $x$ is the number of successes in the sample (may be 0, 1, 2, 3, ...).
- $n$ is the size of the sample or the number of trials.
- $C$ stands for a combination.



**Poisson Distribution**

$P(x) = \frac{\mu^{x}e^{-\mu}}{x!}$

where:
- $\mu$ is the mean number of occurrences (successes) in a particular interval.
- $e$ is the constant 2.71828 (base of the Naperian logarithmic system).
- $x$ is the number of occurrences (successes).
- $P(x)$ is the probability for a specified value of $x$.

**Mean of a Poisson Distribution**

$\mu = n\pi$

where:
- $\pi$ is the probability of success.
- $n$ is the total number of trials.

**Mean of the Uniform Distribution

$\mu = \frac{a + b}{2}$

where:
- $\mu$ is the mean.



**Standard Deviation of the Uniform Distribution**

$\sigma = \sqrt{\frac{(b - a)^2}{12}}$

where:
- $a$ is the minimum value of the interval.
- $b$ is the maximum value of the interval.

**Uniform Distribution Probability

$P(x) = (height)(base) = \frac{1}{b - a}(b - a)$

where:

::::
:::: {.column width="50%"}

- $a \leq x \leq b$, and $P(x) = 0$ elsewhere.
- $a$ is the minimum value of the interval.
- $b$ is the maximum value of the interval.



**Normal Probability Distribution**

$P(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\left[\frac{(x - \mu)^2}{2\sigma^2}\right]}$

where:
- $\sigma$ is the standard deviation.
- $\mu$ is the mean.
- $e$ is a constant (base of the natural logarithm, approximately 2.718).
- $\pi$ is a constant (approximately 3.1416).
- $x$ is the value of the random variable.

**Standard Normal Value (One Observation - $\sigma$ Known)**

$z = \frac{x - \mu}{\sigma}$

This formula is used when calculating the $z$ value for *only one observation* is calculated.


where:
- $z$ denotes the signed distance between a selected value $x$ and the population mean $\mu$, divided by the population standard deviation $\sigma$.
- $x$ is the value of any particular observation or measurement.
- $\mu$ is the mean of the distribution.
- $\sigma$ is the standard deviation of the distribution.


**Continuity Correction Factor**

- If $P(x) \geq x$, then use $(x - 0.5)$.
- If $P(x) > x$, then use $(x + 0.5)$.
- If $P(x) \leq x$, then use $(x + 0.5)$.
- If $P(x) < x$, then use $(x - 0.5)$.

The value 0.5 is subtracted or added to a selected value when a discrete probability distribution is approximated by a continuous probability distribution.

**Exponential Distribution**

$P(x) = \lambda e^{-\lambda x}$

where:
- $\lambda$ is the rate parameter, with $\lambda = \frac{1}{\mu}$ or $\mu = \frac{1}{\lambda}$.
- $e$ is a constant (base of the natural logarithm, approximately 2.718).
- $x$ is the value of the random variable.
- Both the mean ($\mu$) and the standard deviation ($\sigma$) are equal to $\frac{1}{\lambda}$.

::::
:::

-------------- page spacer ------------------


::: {.columns}
:::: {.column width="50%"}

**Probability of Exponential Distribution**

$P(\text{arrival time $< x$}) = 1 - e^{-\lambda x}$

where:
- $\lambda$ is the rate parameter.
- $e$ is a constant (base of the natural logarithm, approximately 2.718).
- $x$ is the value of the random variable.
- Both the mean ($\mu$) and the standard deviation ($\sigma$) are equal to $\frac{1}{\lambda}$.

**Standard Error of the Mean**

$\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$

where:
- $\sigma_{\bar{x}}$ is the standard deviation of the sample means indicated by $\bar{x}$.
- $n$ is the sample size.
- $\sigma$ is the population standard deviation.


**Standard Normal Value (More than One Observation - $\sigma$ Known)**

$z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$

This formula is used in cases where the research refers to a *sample rather than just one observation*.

where:
- $z$ is the distance between a selected value, $\bar{x}$, and the population mean $\mu$, divided by the standard error of the mean $\frac{\sigma}{\sqrt{n}}$.
- $\bar{x}$ is the sample mean.
- $\mu$ is the population mean.
- $\sigma$ is the population standard deviation.
- $n$ is the sample size.



**Confidence Interval for a Population Mean ($\sigma$ Known)**

$\bar{x} \pm z\frac{\sigma}{\sqrt{n}}$

where:
- $z$ is the standardized distance from the mean $\mu$.
- $\bar{x}$ is the sample mean.
- $\sigma$ is the population standard deviation.
- $n$ is the sample size.

**Confidence Interval for a Population Mean ($\sigma$ Unknown)**

$\bar{x} \pm t \frac{s}{\sqrt{n}}$

where:
- $t$ refers to the t distribution.

::::
:::: {.column width="50%"}

- $\bar{x}$ is the sample mean.
- $s$ is the sample standard deviation.
- $n$ is the sample size.

**Sample Proportion**

$p = \frac{x}{n}$

where:
- $p$ is the sample proportion.
- $x$ is the number of successes.
- $n$ is the sample size.

**Confidence Interval for a Population Proportion**

$p \pm z \sqrt{\frac{p(1-p)}{n}}$

where:
- $p$ is the sample proportion, an estimate for the population proportion $\pi$.
- $z$ is the standard distance from the mean $\mu$.
- $n$ is the sample size.

**Sample Size for Estimating the Population Mean**

$E = z\frac{\sigma}{\sqrt{n}} \quad \text{solved for } n \text{ yields} \quad n = \left(\frac{z\sigma}{E}\right)^2$

where:
- $n$ is the sample size.
- $z$ is the standard distance from the mean $\mu$.
- $\sigma$ is the population standard deviation.
- $E$ is the maximum allowable error.


**Sample Size for the Population Proportion**

$E = z\sqrt{\frac{\pi (1 - \pi)}{n}} \quad \text{solved for } n \text{ yields} \quad n = \pi (1-\pi)\left(\frac{z}{E}\right)^2$

where:
- $n$ is the size of the sample.
- $z$ is the standard normal value corresponding to the desired level of confidence.
- $\pi$ is the population proportion.
- $E$ is the maximum allowable error.

**Finite-Population Correction Factor**

$FPC = \sqrt{\frac{N - n}{N - 1}}$

To be used if the sample is a significant part of a finite population.

where:
- $n$ is the size of the sample.
- $N$ is the size of the population.

::::
:::


--------------- page spacer --------------

::: {.columns}
:::: {.column width="50%"}

**Testing a Mean ($\sigma$ Known)**

$z = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$

The $z$ value is based on the sampling distribution of the sample mean $\bar{x}$, which follows the normal distribution with the sampling mean $\mu_{\bar{x}}$ equal to the population mean $\mu$ and a standard error of the mean $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$.

where:
- $z$ is the distance between a selected value, $\bar{x}$, and the mean $\mu$, divided by the standard error of the mean $\frac{\sigma}{\sqrt{n}}$.
- $\bar{x}$ is the sample mean.
- $\mu$ is the population mean.
- $\sigma$ is the population standard deviation.
- $n$ is the sample size.


**Testing a Mean ($\sigma$ Unknown)**

$t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$

with $n-1$ degrees of freedom.

where:
- $\bar{x}$ is the sample mean.
- $\mu$ is the hypothesized population mean.
- $s$ is the sample standard deviation.
- $n$ is the number of observations in the sample.

**Type II Error**

$z = \frac{\bar{x}_{C} - \mu_{1}}{\frac{\sigma}{\sqrt{n}}}$

where:
- $\bar{x}_{C}$ is the sample mean of region C.
- $\mu_{1}$ is the hypothesized population mean of region C.
- $\sigma$ is the population standard deviation.
- $n$ is the number of observations in the sample.

**Two-Sample Test -- Variance of the Distribution of Differences ($\sigma$ Known)**

$\sigma^{2}_{\bar{x_{1}} - \bar{x_{2}}} = \frac{\sigma^{2}_{1}}{n_{1}} + \frac{\sigma^{2}_{2}}{n_{2}}$

where:
- $\sigma^{2}_{\bar{x_{1}} - \bar{x_{2}}}$ is the variance of the differences in means.
- $\bar{x}_{1}$ and $\bar{x}_{2}$ are the sample means of the first and second sample, respectively.
- $n_{1}$ and $n_{2}$ are the sample sizes.


**Two-Sample Test -- Test of Means ($\sigma$ Known)**

$z = \frac{\bar{x}_{1} - \bar{x}_{2}}{\sqrt{\frac{\sigma^{2}_{1}}{n_{1}} + \frac{\sigma^{2}_{2}}{n_{2}}}}$


::::
:::: {.column width="50%"}

This formula applies under the following conditions:
1. The sampled populations are approximately normally distributed.
2. The sampled populations are independent.
3. The standard deviations of the two populations are *known*.

where:
- $z$ is the standard value.
- $\bar{x}_{1}$ and $\bar{x}_{2}$ are the sample means.
- $\sigma^{2}_{1}$ and $\sigma^{2}_{2}$ are the population variances.
- $n_{1}$ and $n_{2}$ are the sample sizes.


**Two-Sample Test -- Pooled Variance ($\sigma$ Unknown)**

$s^{2}_{p} = \frac{(n_{1} - 1)s^{2}_{1} + (n_{2} - 1)s^{2}_{2}}{n_{1} + n_{2} - 2}$

This formula assumes the following conditions:
1. The sampled populations are approximately normally distributed.
2. The sampled populations are independent.
3. The standard deviations of the two populations are *equal*.

The formula computes a weighted mean of the two sample variances using the degrees of freedom that each sample provides. The resulting value serves as an estimate for the unknown population standard deviation. Pooling is used because it assumes the two populations have *equal standard deviations*, and combining the sample information provides the best estimate of the population standard deviation.

where:
- $s^{2}_{p}$ is the pooled variance.
- $s^{2}_{1}$ and $s^{2}_{2}$ are the variances of the first and second samples, respectively.
- $n_{1}$ and $n_{2}$ are the sample sizes of the first and second samples, respectively.
- $n_{1} + n_{2} - 2$ is the degrees of freedom ($df$).


**Two-Sample Test -- Test of Means ($\sigma$ Unknown)**

$t = \frac{\bar{x}_{1} - \bar{x}_{2}}{\sqrt{s^{2}_{p}\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)}}$

Notes
- The pooled variance $s^{2}_{p}$ must be computed beforehand. Prerequisites:
  1. The sampled populations are approximately normally distributed.
  2. The sampled populations are independent.

::::
:::

----------- page spacer ----------------------

::: {.columns}
:::: {.column width="50%"}

  3. The standard deviations of the two populations are **equal**.
- The **Wilcoxon rank-sum test**is an alternative to the Two-Sample t test and does **not**require normal distribution or equal variances.

where:
- $\bar{x}_{1}$: Mean of the first sample.
- $\bar{x}_{2}$: Mean of the second sample.
- $n_{1}$: Number of observations in the first sample.
- $n_{2}$: Number of observations in the second sample.
- $s^{2}_{p}$: Pooled estimate of the population variance.

**Two-Sample Test -- Test Statistic for No Difference in Means, Unequal Variances ($\sigma$ Unknown)**

$t = \frac{\bar{x}_{1} - \bar{x}_{2}}{\sqrt{\frac{s^{2}_{1}}{n_{1}} + \frac{s^{2}_{2}}{n_{2}}}}$

Note
This formula assumes that both sample variances are **unequal**. Unlike the pooled variance approach, the denominator splits into two components using $s^{2}_{1}$ and $s^{2}_{2}$ as variances.

where:
- $\bar{x}_{1}$: Mean of the first sample.
- $\bar{x}_{2}$: Mean of the second sample.
- $s^{2}_{1}$: Sample variance of the first sample.
- $s^{2}_{2}$: Sample variance of the second sample.
- $n_{1}$: Number of observations in the first sample.
- $n_{2}$: Number of observations in the second sample.




**Two-Sample Test -- Degrees of Freedom for Unequal Variance Test ($\sigma$ Unknown)**

$df = \frac{\left[\frac{s^{2}_{1}}{n_{1}} + \frac{s^{2}_{2}}{n_{2}}\right]^{2}}{\frac{\left(\frac{s^{2}_{1}}{n_{1}}\right)^{2}}{n_{1} - 1} + \frac{\left(\frac{s^{2}_{2}}{n_{2}}\right)^{2}}{n_{2} - 1}}$

where:
- $df$ is the degree of freedom.
- $s^{2}_{1}$ is the sample variance of the first sample.
- $s^{2}_{2}$ is the sample variance of the second sample.
- $n_{1}$ is the number of observations in the first sample.
- $n_{2}$ is the number of observations in the second sample.



**Two-Sample Test -- Standard Deviation of Differences**

$s_{d} = \sqrt{\frac{\sum(d - \bar{d})^2}{n - 1}}$

where:
- $\bar{d}$ is the mean of the difference between the paired or related observations.
- $s_{d}$ is the standard deviation of the differences between the paired or related observations.
- $n$ is the number of paired observations.
::::
:::: {.column width="50%"}

**Two-Sample Test -- Paired $t$ Test**

$t = \frac{\bar{d}}{\frac{s_{d}}{\sqrt{n}}}$

This test reduces variation from the sampling distribution compared to independent samples but usually has fewer degrees of freedom ($n-1$). 

An alternative for *dependent samples* is the **Wilcoxon signed-rank test**, which does not require the normality assumption.

where:
- $\bar{d}$ is the mean of the difference between the paired or related observations.
- $s_{d}$ is the standard deviation of the differences between the paired or related observations.
- $n$ is the number of paired observations.



**ANOVA -- Test Statistic for Comparing Two Variances**

$F = \frac{s^{2}_{1}}{s^{2}_{2}}$

where:
- $s^{2}_{1}$ is the sample variance of the first sample.
- $s^{2}_{2}$ is the sample variance of the second sample.
- $F$ represents the F distribution.

If the null hypothesis is true, the test statistic follows the $F$ distribution with $n_{1} - 1$ and $n_{2} - 1$ degrees of freedom.

**ANOVA -- Critical Value for F Statistic**

$F = \frac{k - 1}{n - k}$

where:
- $F$ represents the F distribution.
- $k$ is the number of treatments.
- $n$ is the total number of observations.

In ANOVA, the critical value of the F statistic determines whether the null hypothesis $H_{0}$ can be rejected.



**ANOVA -- Components**

The sum of the squared differences between the following defines the ANOVA terms:

| (a) Each Observation | (b) Overall Mean or Treatment Mean | (c) ANOVA Term          |
|-----------------------|-----------------------------------|-------------------------|
| Overall mean          | Each observation                | **Total variation**    |
| Overall mean          | Each treatment mean             | **Treatment variation**|
| Treatment mean        | Each observation                | **Random variation**   |

::::
:::

---------- page spacer -------------------------



**Assumptions for ANOVA**
- The samples are from independent populations.
- The population variances must be equal.
- The samples are from normal populations.

If these assumptions are not met, the **Kruskal-Wallis**nonparametric test is recommended.



**ANOVA Table (One-Way)**

| Source of Variation | SS       | df       | Mean Square                        | F                 |
|----------------------|----------|----------|------------------------------------|-------------------|
| Treatments           | SST      | $k-1$    | $\frac{SST}{(k-1)} = MST$         | $\frac{MST}{MSE}$ |
| Error                | SSE      | $n-k$    | $\frac{SSE}{(n-k)} = MSE$         |                   |
| Total                | SS Total | $n-1$    |                                    |                   |

where:
- $n$: Total number of observations.
- $k$: Number of treatments.
- $SS$: Sum of squares.
- $SST$: Sum of squares due to treatments, calculated as $\text{SS total} - SSE$ or $\sum{(\bar{x}_C - \bar{x}_G)^2}$, where $\bar{x}_C$ is the treatment mean and $\bar{x}_G$ is the overall mean.
- $SSE$: Sum of squares due to errors, calculated as $\sum{(x - \bar{x}_C)^2}$, where $x$ is each observation and $\bar{x}_C$ is the treatment mean.
- $\text{SS Total}$: Total variation, calculated as $\sum{(x - \bar{x}_G)^2}$, where $x$ is each observation and $\bar{x}_G$ is the overall mean.
- $MST$: Mean square for treatments, an estimate of variance.
- $MSE$: Mean square for errors.

**ANOVA: Confidence Interval for the Difference in Treatment Means**

$(\bar{x}_{1} - \bar{x}_{2}) \pm t\sqrt{\text{MSE}\left(\frac{1}{n_{1}} + \frac{1}{n_{2}}\right)}$

where:
- $\bar{x}_{1}$: Mean of the first sample.
- $\bar{x}_{2}$: Mean of the second sample.
- $t$: t distribution with degrees of freedom equal to $n-k$.
- $MSE$: Mean square error term from the ANOVA table $\left[\frac{SSE}{n-k}\right]$.
- $n_{1}$: Number of observations in the first sample.
- $n_{2}$: Number of observations in the second sample.



**ANOVA: Two-Way -- Sum of Squares Blocks**

$\text{SSB} = k\sum(\bar{x}_{b} - \bar{x}_{G})^{2}$

where:
- $k$: Number of treatments.
- $b$: Number of blocks.
- $\bar{x}_{b}$: Sample mean of block $b$.
- $\bar{x}_{G}$: Overall mean.

----------- page spacer ------------

**ANOVA: Two-Way -- Table*

**ANOVA Table (Two-Way)**

| Source of Variation | SS       | df                   | Mean Square                        | F                 |
|----------------------|----------|----------------------|------------------------------------|-------------------|
| Treatments           | SST      | $k-1$               | $\frac{SST}{(k-1)} = MST$         | $\frac{MST}{MSE}$ |
| Blocks               | SSB      | $b-1$               | $\frac{SSB}{(b-1)} = MSB$         | $\frac{MSB}{MSE}$ |
| Error                | SSE      | $(k-1)(b-1)$        | $\frac{SSE}{(k-1)(b-1)} = MSE$    |                   |
| Total                | SS Total | $n-1$               |                                    |                   |

where:
- $n$: Total number of observations.
- $k$: Number of treatments.
- $SS$: Sum of squares.
- $SST$: Sum of squares due to treatments, calculated as $\sum{(\bar{x}_C - \bar{x}_G)^2}$ or $\text{SS Total} - SSE - SSB$.
- $SSB$: Sum of squares blocks, calculated as $k\sum{(\bar{x}_{b} - \bar{x}_{G})^2}$, where $\bar{x}_b$ is the block mean and $\bar{x}_G$ is the overall mean.
- $SSE$: Sum of squares due to errors, calculated as $\sum{(x - \bar{x}_C)^2}$, where $\bar{x}_C$ is the treatment mean.
- $\text{SS Total}$: Total variation, calculated as $\sum{(x - \bar{x}_G)^2}$, where $x$ is each sample observation and $\bar{x}_G$ is the overall mean.
- $MST$: Mean square for treatments, an estimate of variance.
- $MSB$: Mean square for blocks.
- $MSE$: Mean square for errors.


**Illustration of Two-Way ANOVA**

The following is an illustration of how the different variations relate:

![Illustration of a Two-Way ANOVA](./img/anova.png)

-------------- page spacer --------------

**ANOVA: Two-Way with Interaction (Factors) -- Table

ANOVA Table with Interaction (Factors)

| Source of Variation | SS       | df                   | Mean Square                        | F                 |
|----------------------|----------|----------------------|------------------------------------|-------------------|
| Factor A            | SSA      | $k-1$               | $\frac{SSA}{(k-1)} = MSA$         | $\frac{MSA}{MSE}$ |
| Factor B            | SSB      | $b-1$               | $\frac{SSB}{(b-1)} = MSB$         | $\frac{MSB}{MSE}$ |
| Interaction         | SSI      | $(k-1)(b-1)$        | $\frac{SSI}{(k-1)(b-1)} = MSI$    | $\frac{MSI}{MSE}$ |
| Error               | SSE      | $n-kb$              | $\frac{SSE}{(n-kb)} = MSE$        |                   |
| Total               | SS Total | $n-1$               |                                    |                   |

-------------- page spacer -------------

::: {.columns}
:::: {.column width="50%"}

**Linear Regression: Correlation Coefficient**

$r = \frac{\sum(x-\bar{x})(y-\bar{y})}{(n-1)s_{x}s_{y}}$

Subject
A measure of the strength of the linear relationship between two variables, ranging from $-1$ to $+1$.

Notes
- The correlation coefficient is independent of scale if $\sum(x-\bar{x})(y-\bar{y})$ is divided by $s_{x}$ and $s_{y}$.
- It becomes independent of sample size when divided by $(n-1)$.
- Correlation between two independent variables is uncritical if $-0.70 < r < 0.70$. For a more precise test, use the variance inflation factor (VIF).

where:
- $r$: Correlation coefficient.
- $x$: Variable value of the x population.
- $y$: Variable value of the y population.
- $\bar{x}$: Mean of variable values in the x population.
- $\bar{y}$: Mean of variable values in the y population.
- $n$: Number of observations in the sample.
- $(n-1)$: Degrees of freedom.
- $s_{x}$: Standard deviation of the x population.
- $s_{y}$: Standard deviation of the y population.



**Linear Regression: t Test for the Correlation Coefficient $r$**

$t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}}$

Subject:
Resolves the question about whether there could be zero correlation in the population from which the sample was selected.

where
- $r$: Correlation coefficient.
- $n$: Number of observations in the sample.

**Linear Regression Equation: General Form

$\hat{y} = a + bx$

Subject
An equation that expresses the linear relationship between two variables.

where:
- $\hat{y}$: Estimated value of the $y$ variable for a selected $x$ value.
- $a$: Y-intercept. It is the estimated value of $y$ when $x = 0$.
- $b$: Slope of the line, or the average change in $\hat{y}$ for each change of one unit (either increase or decrease) in the independent variable $x$.
- $x$: Any value of the independent variable that is selected.


**Linear Regression: Slope of Regression Line**

$b = r\left(\frac{s_{y}}{s_{x}}\right)$

where:
- $r$: Correlation coefficient.

::::
:::: {.column width="50%"}

- $s_{y}$: Standard deviation of $y$ (the dependent variable).
- $s_{x}$: Standard deviation of $x$ (the independent variable).

**Linear Regression: Y-Intercept**

$a = \bar{y} - b\bar{x}$

where:
- $\bar{y}$: Mean of $y$ (the dependent variable).
- $\bar{x}$: Mean of $x$ (the independent variable).


**Linear Regression: t Test for the Slope $b$**

$t = \frac{b - 0}{s_{b}}$

Subject:
Conducts a test on whether the slope of the regression line is different from zero. In such a circumstance, we can reasonably conclude that the regression line adds to the predictive ability of the regression equation.

where:
- $b$: Estimate of the regression line's slope calculated from the sample information.
- $s_{b}$: Standard error of the slope estimate, also determined from sample information.
- Degrees of freedom: $n-2$.

**Linear Regression: Standard Error of Estimate**

$s_{y \cdotp x} = \sqrt{\frac{\sum(y-\hat{y})^2}{n-2}} = \sqrt{\frac{\text{SSE}}{n-2}}$

Subject:
A relative measure of a regression equation's ability to predict.

where:
- $s_{y \cdotp x}$: Standard error of estimate with $y \cdotp x$ interpreted as the standard error of $y$ for a given value of $x$. It is analogous to the standard deviation, measuring dispersion around a mean.
- $y$: Observed value.
- $\hat{y}$: Predicted value.
- $\sum(y-\hat{y})^2$: Sum of squares error (SSE) or residuals.



**Linear Regression: Coefficient of Determination**

$r^2 = \frac{\text{SSR}}{\text{SS Total}} = \frac{\sum(\hat{y}-\bar{y})^2}{\sum(y-\bar{y})^2} = 1 - \frac{\text{SSE}}{\text{SS Total}} = 1 - \frac{\sum(y-\hat{y})^2}{\sum(y-\bar{y})^2}$

Subject:
The proportion of the total variation in the dependent variable $y$ that is explained, or accounted for, by the variation in the independent variable $x$.

where:
- **SS Total**: Total variation (sum of squares total).
- **SSR**: Sum of squares regression.
- **SSE**: Sum of squares errors or residuals.

::::
:::


::: {.columns}
:::: {.column width="50%"}

**Linear Regression: Confidence Interval for the Mean of $y$ Given $x$**

$\hat{y} \pm t s_{y \cdotp x}\sqrt{\frac{1}{n}+\frac{(x-\bar{x})^2}{\sum(x-\bar{x})^2}}$

Subject:
Applied when the regression equation is used to predict the *mean value of $y$* for a given value of $x$.

where:
- $x$: Given value.
- $\bar{x}$: Sample mean.
- $\hat{y}$: Predicted value.
- $s_{y \cdotp x}$: Standard error of estimate, interpreted as the standard error of $y$ for a given value of $x$. Similar to the standard deviation, measuring dispersion around a mean.

**Linear Regression: Prediction Interval for $y$ Given $x$**

$\hat{y} \pm t s_{y \cdotp x}\sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^2}{\sum(x-\bar{x})^2}}$

Subject:
Applied when the regression equation is used to predict an *individual value of $y$ ($n = 1$)* for a given value of $x$. Refer to the confidence interval equation for comparison.

where:
- $x$: Given value.
- $\bar{x}$: Sample mean.
- $\hat{y}$: Predicted value.
- $s_{y \cdotp x}$: Standard error of estimate, interpreted as the standard error of $y$ for a given value of $x$. Similar to the standard deviation, measuring dispersion around a mean.


**Multiple Regression: General Equation**

$\hat{y} = a + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} + ... + b_{k}x_{k}$

Subject:
Enhanced equation of the linear regression formula for more than one dependent variable.

where:
- $a$: Intercept, the value of $\hat{y}$ when all the $x$'s are zero.
- $b_{j}$: The amount by which $\hat{y}$ changes when that particular $x_{j}$ increases by one unit, with the values of all other independent variables held constant.
- $k$: Number of independent variables.

**Multiple Regression: Standard Error of Estimate**

$s_{y \cdotp 123 \cdotp k} = \sqrt{\frac{\sum(y-\hat{y})^2}{n-(k+1)}} = \sqrt{\frac{\text{SSE}}{n-(k+1)}} = \sqrt{\text MSE}$

Subject:
A relative measure of a regression equation's ability to predict for more than one independent variable.

where:
- $y$: Actual observation.
- $\hat{y}$: Estimated value computed from the regression equation.

::::
:::: {.column width="50%"}

- $n$: Number of observations in the sample.
- $k$: Number of independent variables.
- $SSE$: Residual sum of squares from an ANOVA table, equal to $\sum(y-\hat{y})^2$.



**Multiple Regression ANOVA -- Table**

**Multiple Regression ANOVA Table**

| Source             | SS       | df               | MS                          | F                 |
|---------------------|----------|------------------|-----------------------------|-------------------|
| Regression          | SSR      | $k$             | $MSR = \frac{SSR}{k}$       | $\frac{MSR}{MSE}$ |
| Residual or error   | SSE      | $n-(k+1)$       | $MSE = \frac{SSE}{n-(k+1)}$ |                   |
| Total               | SS Total | $n-1$           |                             |                   |

**Multiple Regression: Coefficient of Multiple Determination**

$R^2 = \frac{\text{SSR}}{\text{SS Total}}$

Subject:
The percent of variation in the dependent variable, $y$, explained by the set of independent variables, $x_{1}$, $x_{2}$, $x_{3}$, ... $x_{k}$.

where:
- **SS Total**: Total variation (sum of squares total).
- **SSR**: Sum of squares regression.

**Multiple Regression: Adjusted Coefficient of Multiple Determination**

$R^2_{adj} = \frac{\frac{SSE}{n-(k+1)}}{\frac{\text{SS Total}}{n-1}}$

Subject:
The percent of variation in the dependent variable, $y$, explained by the set of independent variables, $x_{1}$, $x_{2}$, $x_{3}$, ... $x_{k}$. 

As more independent variables are added to the multiple regression model, $R^2$ tends to increase. If the number of variables, $k$, and the sample size, $n$, are equal, the coefficient of determination becomes 1. To avoid this trend, $R^2$ is adjusted.

where:
- **SS Total**: Total variation (sum of squares total).
- **SSE**: Sum of squares error or residual.

**Multiple Regression: Global Test**

$F = \frac{\frac{\text{SSR}}{k}}{\frac{\text{SSE}}{n-(k+1)}}$

Subject:
The Global Test investigates whether it is possible that all the independent variables have zero regression coefficients. It compares the sum of squares regression per unit of sum of squares residuals. The higher the explained variances compared to the residual variances, the more positive the value of the F distribution.

where:
- **SSR**: Sum of squares regression.

::::
:::

------------- page spacer -----------------

::: {.columns}
:::: {.column width="50%"}

- **SSE**: Sum of squares error or residual.
- $n$: Number of observations in the sample.
- $k$: Number of independent variables.

**Multiple Regression: t Test Individual Coefficients $b$**

$t = \frac{b_{j} - 0}{s_{b_{j}}}$

Subject:
Tests the independent variables individually to determine whether the regression coefficients differ from zero. If a regression coefficient is likely to be zero, it does not contribute to the regression equation's ability to predict.

where:
- $b_{j}$: Any one of the regression coefficients.
- $s_{b_{j}}$: Standard error of the slope estimate, determined from sample information.


**Multiple Regression: Variance Inflation Factor**

$VIF = \frac{1}{1 - R^2_{j}}$

Subject:
A VIF greater than 10 is considered unsatisfactory, indicating that the independent variable should be removed from the analysis.

where:
- $R^2_{j}$: Coefficient of determination.

**Test of Hypothesis: One Proportion**

$z = \frac{p - \pi}{\sqrt{\frac{\pi(1 - \pi)}{n}}}$

where:
- $\pi$: Population proportion.
- $p$: Sample proportion.
- $n$: Sample size.

**Test of Hypothesis: Pooled Proportion**

$p_c = \frac{x_1 + x_2}{n_1 + n_2}$

where:
- $p_{c}$: Pooled proportion possessing the trait in the combined samples (pooled estimate of the population proportion).
- $x_1$: Number possessing the trait in the first sample.
- $x_2$: Number possessing the trait in the second sample.
- $n_{1}$: Number of observations in the first sample.
- $n_{2}$: Number of observations in the second sample.


**Test of Hypothesis: Two-Sample Test of Proportions**

$z = \frac{p_{1} - p_{2}}{\sqrt{\frac{p_{c}(1 - p_{c})}{n_{1}} + \frac{p_{c}(1 - p_{c})}{n_{2}}}}$

where:
- $n_{1}$: Number of observations in the first sample.

::::
:::: {.column width="50%"}

- $n_{2}$: Number of observations in the second sample.
- $p_{1}$: Proportion in the first sample possessing the trait.
- $p_{2}$: Proportion in the second sample possessing the trait.
- $p_{c}$: Pooled proportion possessing the trait in the combined samples (pooled estimate of the population proportion).

**Chi-Square Test Statistic**

$\chi^2 = \sum\left[\frac{(f_o - f_e)^2}{f_e}\right]$

Degrees of Freedom
$k - 1$

where:
- $k$: Number of categories.
- $f_o$: Observed frequency in a particular category.
- $f_e$: Expected frequency in a particular category.


**Expected Frequency**

$f_e = \frac{\text{(row total) \text{(column total)}}}{\text{(grand total)}}$

where:
- $f_e$: Expected frequency in a particular category.

Sign Test: $n > 10$

$z = \frac{(x \pm 0.50) - \mu}{\sigma}$

where:
- $z$: Standard value.
- $\pm 0.50$: Continuity correction factor.
- $x$: Number of plus ($+$) or minus ($-$) signs.
- $\mu$: Population mean.
- $\sigma$: Population standard deviation.

**Sign Test: $n > 10$, + Signs More Than $n/2$**

$z = \frac{(x - 0.50) - \mu}{\sigma} = \frac{(x - 0.50) - 0.50n}{0.50\sqrt{n}}$

where:

::::
:::

----------------- page spacer ----------------------

::: {.columns}
:::: {.column width="50%"}

- $z$: Standard value.
- $\pm 0.50$: Continuity correction factor.
- $x$: Number of plus ($+$) or minus ($-$) signs.
- $n$: Sample size.
- $\mu$: Population mean.
- $\sigma$: Population standard deviation.


**Sign Test: $n > 10$, + Signs **Less**Than $n/2$**

$z = \frac{(x + 0.50) - \mu}{\sigma} = \frac{(x + 0.50) - 0.50n}{0.50\sqrt{n}}$

where:
- $z$: Standard value.
- $\pm 0.50$: Continuity correction factor.
- $x$: Number of plus ($+$) or minus ($-$) signs.
- $n$: Sample size.
- $\mu$: Population mean.
- $\sigma$: Population standard deviation.

**Wilcoxon Rank-Sum Test**

$z = \frac{W - \frac{n_1(n_1 + n_2 + 1)}{2}}{\sqrt{\frac{n_1n_2(n_1 + n_2 + 1)}{12}}}$

Subject:
This test is specifically designed to determine whether two *independent samples* came from equivalent populations.

Note
This test is an alternative to the Two-Sample t test but does **not**require that the two populations follow the normal distribution or have equal population variances.

where:
- $n_1$: Number of observations of the first population.
- $n_2$: Number of observations of the second population.
- $W$: Sum of the ranks from the first population.


**Kruskal-Wallis Test**

$H = \frac{12}{n(n + 1)}\left[\frac{(\sum R_1)^2}{n_1} + \frac{(\sum R_2)^2}{n_2} + ... + \frac{(\sum R_k)^2}{n_k}\right] - 3(n+1)$

Note
For the Kruskal-Wallis test to be applied, the samples selected from the populations must be independent. If the following prerequisites are met, an ANOVA analysis can be applied instead:
- The samples are from independent populations.
- The population variances must be equal.
- The samples are from normal populations.

**Degrees of Freedom**
$k-1$, where $k$ is the number of populations.

where:
- $\sum R_1, \sum R_2, ..., \sum R_k$: Sums of the ranks of samples 1, 2, ..., $k$.
- $n_1, n_2, ..., n_k$: Sizes of samples 1, 2, ..., $k$.
- $n$: Combined number of observations for all samples.

**Spearman's Coefficient of Rank Correlation**

$r_s = 1 - \frac{6\sum d^2}{n(n^2 - 1)}$

where:
- $r_s$: Spearman's coefficient of rank correlation.
- $d$: Difference between the ranks for each pair.
- $n$: Number of paired observations.

**Hypothesis Test: Rank Correlation**

$t = r_s\sqrt{\frac{n - 2}{1 - r_s^2}}$

where:

::::
:::: {.column width="50%"}

- $r_s$: Spearman's coefficient of rank correlation.

**Index Numbers: Simple Index**

$P = \frac{p_t}{p_0} \times 100$

where:
- $p_0$: Base-period price.
- $p_t$: Given period price.

**Index Numbers: Simple Average of the Price Relatives**

$P = \frac{\sum P_i}{n}$

Note
- **Advantage**: Simple price indices are not dependent on the unit of measure of the item quantified.
- **Disadvantage**: Simple price indices do not account for the relative importance of the items included.

The lack of consideration for the relative importance of items is addressed by the Laspeyres price index and the Paasche price index.

where:
- $P_i$: Simple index for each of the items.
- $n$: Number of items.


**Index Numbers: Simple Aggregate Index**

$P = \frac{\sum p_t}{\sum p_0} \times 100$

Note
Since the aggregate index is influenced by the unit of measure, it is not used frequently.

where:
- $p_0$: Base-period price.
- $p_t$: Given period price.

**Index Numbers: Laspeyres Price Index**

$P = \frac{\sum p_t q_0}{\sum p_0 q_0} \times 100$

Note
The Laspeyres price index assumes that the base-period quantities still have a significant bearing on the current price index and are realistic. This contrasts with the assumption of the Paasche price index.

where:
- $P$: Price index.
- $p_t$: Current price.
- $p_0$: Price in the base period.
- $q_0$: Quantity used in the base period.

**Index Numbers: Paasche Price Index**

$P = \frac{\sum p_t q_t}{\sum p_0 q_t} \times 100$

Note
The Paasche price index assumes current period quantity levels as the base to account for changed preferences in consumed quantities. This contrasts with the assumption of the Laspeyres price index.

where:

::::
:::

--------------- page spacer ---------------------

::: {.columns}
:::: {.column width="50%"}

- $P$: Price index.
- $p_t$: Current price.
- $p_0$: Price in the base period.
- $q_t$: Quantity used in the current period.


**Index Numbers: Fisher's Ideal Index**

$\text{Fisher's Ideal Index} = \sqrt{\text{(Laspeyres index)}\text{(Paasche index)}}$

Note
Fisher's Ideal Index is a geometric mean of the Laspeyres and Paasche price indices.

**Index Numbers: Value Index**

$V = \frac{\sum p_t q_t}{\sum p_0 q_0} \times 100$

where:
- $P$: Price index.
- $p_t$: Current price.
- $p_0$: Price in the base period.
- $q_0$: Quantity used in the base period.

**Index Numbers: Real Income**

$\text{Real income} = \frac{\text{Money income}}{\text{CPI}} \times 100$

where:
- $CPI$: Consumer price index.

**Index Numbers: Index as a Deflator**

$\text{Deflated sales} = \frac{\text{Actual sales}}{\text{An appropriate index}} \times 100$


**Index Numbers: Index for Purchasing Power**

$\text{Purchasing power of dollar} = \frac{\text{\$1}}{\text{CPI}} \times 100$

**Time Series & Forecasting: Linear Trend Equation**

$\hat{y} = a + bt$

where:
- $\hat{y}$: Projected value of the $y$ variable for a selected value of $t$.
- $a$: Y-intercept, the estimated value of $y$ when $t = 0$.
- $b$: Slope of the line, or the average change in $\hat{y}$ for each increase of one unit in $t$.
- $t$: Any value of time that is selected.

**Time Series & Forecasting: Log Trend Equation**

$\log \hat{y} = \log a + \log b(t)$

::::
:::: {.column width="50%"}

**Time Series & Forecasting: Correction Factor for Adjusting Quarterly Means**
$\text{Correction factor} = \frac{4.00}{\text{Total of four means}}$

**Time Series & Forecasting: Durbin-Watson Statistic**

$d = \frac{\sum_{t = 2}^{n} (e_t - e_{t-1})^2}{\sum_{t = 1}^n(e_t)^2}$


::::
:::



LaTeX Template: Two Column Colour Article

Source: http://www.howtotex.com/
Feel free to distribute this template, but please keep the
referal to howtotex.com.
Date: Feb 2011

